# %% use conda env: new_base
import os
import re
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
from icalendar import Calendar
from datetime import datetime, timedelta
import recurring_ical_events
import pytz

from Self_Tracking.calendar_analysis_tests_graphs import (
    debug_sleep_patterns,
    run_tests,
)  # ; run_tests(); debug_sleep_patterns(df)

color_map = {
    "Things": "grey",
    "Meals, Supplements, Sleep": "green",
    "Waste Time": "red",
    "clark.benham@gmail.com": "blue",
    "cb5ye@virginia.edu": "blue",  # anki used to be here but was moved over
}


def parse_ics_files(directory, start_date, end_date):
    """This drops  Personal Dates and the Transparent Events that Google automatically adds (some mistakes, probably not too bad)"""
    all_events = []
    directory = os.path.expanduser(directory)
    ics_files = [
        f for f in os.listdir(directory) if f.endswith(".ics") and "Personal Dates" not in f
    ]
    for ics_file in ics_files:
        calendar_name = os.path.splitext(ics_file)[0]
        file_path = os.path.join(directory, ics_file)
        try:
            with open(file_path, "rb") as f:
                cal = Calendar.from_ical(f.read())
                events = recurring_ical_events.of(cal).between(start_date, end_date)
                for event in events:
                    if event.get("status") and str(event.get("status")).upper() == "CANCELLED":
                        continue
                    if (
                        event.get("TRANSP")
                        and str(event.get("TRANSP")).upper() == "TRANSPARENT"
                        and "clark.benham" in calendar_name
                    ):
                        # print("skipped since google autogenerated", str(event.get("summary")))
                        continue
                    dtstart = event.get("dtstart").dt
                    dtend = (
                        event.get("dtend").dt
                        if event.get("dtend")
                        else dtstart + timedelta(hours=1)
                    )
                    duration = (dtend - dtstart).total_seconds() / 3600.0
                    all_events.append(
                        {
                            "event_name": str(event.get("summary")),
                            "calendar_name": calendar_name,
                            "start_time": dtstart,
                            "end_time": dtend,
                            "duration": duration,
                            "metadata": {
                                k: str(event.get(k))
                                for k in event.keys()
                                if k not in ["dtstart", "dtend", "summary"]
                            },
                        }
                    )
        except Exception as e:
            print(f"Error processing {ics_file}: {e}")
    df = pd.DataFrame(all_events)
    if not df.empty:
        for col in ["start_time", "end_time"]:
            df[col] = pd.to_datetime(df[col], utc=True)
    return df


def process_sleep_events(df):
    df = df.copy()
    bed_df = df[df["event_name"].str.lower() == "bed"].sort_values("start_time")
    woke_df = df[df["event_name"].str.lower() == "woke up"].sort_values("start_time")
    sleep_events = []
    for i, bed_row in bed_df.iterrows():
        bed_time = bed_row["start_time"]
        woke_candidates = woke_df[woke_df["start_time"] > bed_time]
        if woke_candidates.empty:
            continue
        woke_time = woke_candidates.iloc[0]["start_time"]
        base_duration = (woke_time - bed_time).total_seconds() / 3600.0
        intervening = df[(df["start_time"] > bed_time) & (df["end_time"] < woke_time)]
        subtract_duration = intervening["duration"].sum()
        sleep_duration = base_duration - subtract_duration
        sleep_events.append(
            {
                "event_name": "sleep",
                "calendar_name": "Meals, Supplements, Sleep",
                "start_time": bed_time,
                "end_time": woke_time,
                "duration": sleep_duration,
                "metadata": {"subtracted": subtract_duration},
            }
        )
    df = df[~df["event_name"].str.lower().isin(["bed", "woke up"])]
    sleep_df = pd.DataFrame(sleep_events)
    df = pd.concat([df, sleep_df], ignore_index=True)
    return df


def process_slash_events(df):
    df = df.copy()
    new_rows = []
    drop_indices = []
    # Special cases: "knee/gym" becomes "gym" and "Anki/mental math" to anki
    no_slash = {"knee/gym": "gym", "anki/mental math": "anki"}
    for idx, row in df.iterrows():
        name = row["event_name"]
        if "/" in name:
            if name.lower() in no_slash:
                df.at[idx, "event_name"] = no_slash[name.lower()]
                if isinstance(row["metadata"], dict):
                    row["metadata"]["original_name"] = name
                else:
                    row["metadata"] = {"original_name": name}
            else:
                parts = [p.strip() for p in name.split("/")]
                n = len(parts)
                orig_start = row["start_time"]
                orig_end = row["end_time"]
                total_seconds = (orig_end - orig_start).total_seconds()
                segment_seconds = total_seconds / n
                for i, part in enumerate(parts):
                    new_row = row.copy()
                    new_row["event_name"] = part
                    new_row["start_time"] = orig_start + timedelta(seconds=i * segment_seconds)
                    new_row["end_time"] = orig_start + timedelta(seconds=(i + 1) * segment_seconds)
                    new_row["duration"] = segment_seconds / 3600.0  # convert to hours
                    if isinstance(new_row["metadata"], dict):
                        new_row["metadata"]["original_name"] = name
                    else:
                        new_row["metadata"] = {"original_name": name}
                    new_rows.append(new_row)
                drop_indices.append(idx)
    df = df.drop(drop_indices)
    if new_rows:
        df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)
    return df


def process_book_events(df):
    """this won't handle books with numbers in their title: eg `Kings 12` would be `k1`
    TODO would be to standardize this.
    """
    df = df.copy()
    for idx, row in df.iterrows():
        name = row["event_name"]
        # Process only events that start with "book:".
        if name.lower().startswith("book:"):
            remainder = name[len("book:") :].strip()
            # If the remainder has more than one word, create an abbreviation.
            if " " in remainder:
                abbrev = "".join([w[0] for w in remainder.split()]).lower()
                new_name = "book:" + abbrev  # no space after colon
                df.at[idx, "event_name"] = new_name
                if isinstance(row["metadata"], dict):
                    row["metadata"]["full_title"] = remainder
                else:
                    df.at[idx, "metadata"] = {"full_title": remainder}
            else:
                # remove extra spaces: `book: abc`
                df.loc[idx, "event_name"] = name.replace(": ", ":")
    return df


def process_overlaps(df):
    df = df.copy().reset_index(drop=True)
    events = df.sort_values("start_time").reset_index(drop=True)
    boundaries = sorted(set(events["start_time"].tolist() + events["end_time"].tolist()))
    alloc = [0] * len(events)
    for i in range(len(boundaries) - 1):
        seg_start = boundaries[i]
        seg_end = boundaries[i + 1]
        seg_dur = (seg_end - seg_start).total_seconds() / 3600.0
        active = events[
            (events["start_time"] <= seg_start) & (events["end_time"] >= seg_end)
        ].index.tolist()
        if active:
            share = seg_dur / len(active)
            for j in active:
                alloc[j] += share
    events["adjusted_duration"] = alloc
    events["duration"] = events["adjusted_duration"]
    del events["adjusted_duration"]
    return events


# === New Feature Functions ===


# 1. Weekly, Monthly, and Overall Time Summary by Category
def _utc_date(dt):
    """Return a timezone-aware datetime in UTC."""
    if dt.tzinfo is None:
        return dt.replace(tzinfo=pytz.UTC)
    return dt


def overall_time_summary(df, start_date, end_date, split_on_colon=False):
    start_date = _utc_date(start_date)
    end_date = _utc_date(end_date)

    # Filter by dates.
    df_filtered = df[(df["start_time"] >= start_date) & (df["end_time"] <= end_date)].copy()

    # Optionally, filter out implausible book events.
    # For example, if a book event lasts more than 7 hours, it is likely an error.
    df_filtered = df_filtered[
        ~(
            (df_filtered["event_name"].str.lower().str.startswith("book:"))
            & (df_filtered["duration"] > 7)
        )
    ]

    # Group by event_name and compute total time and count of separate entries.
    if split_on_colon:
        df_filtered["event_name_lower"] = (
            df["event_name"].str.lower().apply(lambda s: s.split(":")[0])
        )
    else:
        df_filtered["event_name_lower"] = df_filtered["event_name"].str.lower()

    summary = (
        df_filtered.groupby("event_name_lower")
        .agg(total_time=("duration", "sum"), count=("duration", "count"))
        .reset_index()
    )

    # Top 20 by total time.
    summary_hours = summary.sort_values("total_time", ascending=False).head(20)
    # Top 20 by count of entries.
    summary_count = summary.sort_values("count", ascending=False).head(20)

    print("Overall Time Summary (Top 20 by Hours):")
    for _, row in summary_hours.iterrows():
        print(f"  {row['event_name_lower']}: {row['total_time']:.2f} hours, {row['count']} entries")

    print("\nOverall Time Summary (Top 20 by Entry Count):")
    for _, row in summary_count.iterrows():
        print(f"  {row['event_name_lower']}: {row['count']} entries, {row['total_time']:.2f} hours")

    return summary_hours, summary_count


def weekly_top_events(df, start_date, end_date, top_n=10):
    start_date = _utc_date(start_date)
    end_date = _utc_date(end_date)
    df_filtered = df[(df["start_time"] >= start_date) & (df["end_time"] <= end_date)].copy()
    # Create a week column (the starting date of the week)
    df_filtered["week"] = df_filtered["start_time"].dt.to_period("W").apply(lambda r: r.start_time)

    overall = (
        df_filtered.groupby(["week", "event_name"])
        .agg(total_time=("duration", "sum"), count=("duration", "count"))
        .reset_index()
    )
    overall_top = (
        overall.groupby("week")
        .apply(lambda g: g.sort_values("total_time", ascending=False).head(top_n))
        .reset_index(drop=True)
    )

    by_calendar = (
        df_filtered.groupby(["week", "calendar_name", "event_name"])
        .agg(total_time=("duration", "sum"), count=("duration", "count"))
        .reset_index()
    )
    calendar_top = (
        by_calendar.groupby(["week", "calendar_name"])
        .apply(lambda g: g.sort_values("total_time", ascending=False).head(top_n))
        .reset_index(drop=True)
    )

    print("Weekly Top Events Overall:")
    print(overall_top)
    print("\nWeekly Top Events by Calendar:")
    print(calendar_top)
    return overall_top, calendar_top


def monthly_top_events(df, start_date, end_date, top_n=10):
    start_date = _utc_date(start_date)
    end_date = _utc_date(end_date)
    df_filtered = df[(df["start_time"] >= start_date) & (df["end_time"] <= end_date)].copy()
    # Create a month column (the starting date of the month)
    df_filtered["month"] = df_filtered["start_time"].dt.to_period("M").apply(lambda r: r.start_time)

    overall = (
        df_filtered.groupby(["month", "event_name"])
        .agg(total_time=("duration", "sum"), count=("duration", "count"))
        .reset_index()
    )
    overall_top = (
        overall.groupby("month")
        .apply(lambda g: g.sort_values("total_time", ascending=False).head(top_n))
        .reset_index(drop=True)
    )

    by_calendar = (
        df_filtered.groupby(["month", "calendar_name", "event_name"])
        .agg(total_time=("duration", "sum"), count=("duration", "count"))
        .reset_index()
    )
    calendar_top = (
        by_calendar.groupby(["month", "calendar_name"])
        .apply(lambda g: g.sort_values("total_time", ascending=False).head(top_n))
        .reset_index(drop=True)
    )

    print("Monthly Top Events Overall:")
    print(overall_top)
    print("\nMonthly Top Events by Calendar:")
    print(calendar_top)
    return overall_top, calendar_top


def graph_waste_days(df):
    df = df.copy()
    # Define waste events as those whose event_name contains 'waste' (case-insensitive)
    df_waste = df[df["calendar_name"] == "Waste Time"].copy()
    df_waste["date"] = df_waste["start_time"].dt.date
    days = []
    for date_val, group in df_waste.groupby("date"):
        group = group.sort_values("start_time")
        current_start = None
        current_end = None
        max_block = 0
        for _, row in group.iterrows():
            if current_start is None:
                current_start = row["start_time"]
                current_end = row["end_time"]
            else:
                gap = (row["start_time"] - current_end).total_seconds() / 60.0
                if gap <= 30:
                    current_end = max(current_end, row["end_time"])
                else:
                    block_duration = (current_end - current_start).total_seconds() / 3600.0
                    max_block = max(max_block, block_duration)
                    current_start = row["start_time"]
                    current_end = row["end_time"]
        if current_start is not None:
            block_duration = (current_end - current_start).total_seconds() / 3600.0
            max_block = max(max_block, block_duration)
        days.append({"date": date_val, "max_waste_block": max_block})
    df_days = pd.DataFrame(days)
    df_days["waste_flag"] = df_days["max_waste_block"] >= 4
    plt.figure(figsize=(10, 5))
    plt.bar(df_days["date"], df_days["waste_flag"].astype(int))
    plt.title("Days with ≥4 Hours Contiguous Waste")
    plt.xlabel("Date")
    plt.ylabel("1 if wasted ≥4 hours continuously")
    plt.ylim(bottom=0)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()


def graph_gym_count(df, average_for=1):
    # Filter for gym events.
    df_gym = df[df["event_name"].str.lower().str.contains("gym")].copy()
    df_gym["date"] = pd.to_datetime(df_gym["start_time"].dt.date)
    # Group by date: count events and sum durations (assumed in hours)
    daily_agg = df_gym.groupby("date").agg(count=("date", "size"), hours=("duration", "sum"))
    # Create a full date range.
    full_index = pd.date_range(start=daily_agg.index.min(), end=daily_agg.index.max(), freq="D")
    daily_agg = daily_agg.reindex(full_index, fill_value=0)
    daily_agg.index.name = "date"
    daily_agg = daily_agg.reset_index()

    if average_for > 1:
        daily_agg = daily_agg.set_index("date").resample(f"{average_for}D").mean().reset_index()

    plt.figure(figsize=(10, 5))
    plt.scatter(daily_agg["date"], daily_agg["count"])
    plt.title(f"Daily 'Gym' Entry Count (averaged over {average_for} days)")
    plt.xlabel("Date")
    plt.ylabel("Average Count")
    plt.ylim(bottom=0)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    return daily_agg


def graph_drink_count(df):
    return graph_str_count(df, s="drink", exact_match=True)


def graph_str_count(df, s, exact_match=False):
    df = df.copy()
    if exact_match:
        mask = df["event_name"].str.lower() == s.lower()
    else:
        mask = df["event_name"].str.lower().str.contains(s, na=False)
    df_drink = df[mask]
    if df_drink.empty:
        print(f"No events matched '{s}'.")
        return
    df_drink["date"] = df_drink["start_time"].dt.date
    daily_count = df_drink.groupby("date").size().reset_index(name="count")
    plt.figure(figsize=(10, 5))
    plt.bar(daily_count["date"], daily_count["count"])
    plt.title(f"Daily '{s}' Entry Count")
    plt.xlabel("Date")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.ylim(bottom=0)
    plt.show()


def scatter_calendar_time(df, average_for=1):
    df = df.copy()
    df["date"] = pd.to_datetime(df["start_time"].dt.date)
    calendars = df["calendar_name"].unique()
    results = {}
    for cal in calendars:
        df_cal = df[df["calendar_name"] == cal].copy()
        daily = df_cal.groupby("date")["duration"].sum()
        full_index = pd.date_range(start=daily.index.min(), end=daily.index.max(), freq="D")
        daily = daily.reindex(full_index, fill_value=0)
        daily.index.name = "date"
        daily = daily.to_frame(name="total_time")
        if average_for > 1:
            daily = daily.resample(f"{average_for}D").mean()
        daily = daily.reset_index().rename(columns={"index": "date"})
        plt.figure(figsize=(10, 5))
        plt.scatter(daily["date"], daily["total_time"])
        plt.title(f"Daily Time Spent in {cal} (averaged over {average_for} days)")
        plt.xlabel("Date")
        plt.ylabel("Total Time (hours)")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        results[cal] = daily
    return results


def graph_sleep_nap(df, average_for=1):
    import pandas as pd
    import matplotlib.pyplot as plt

    df = df.copy()
    # Ensure date is a proper datetime (date only)
    df["date"] = pd.to_datetime(df["start_time"].dt.date)

    # Separate sleep and nap events.
    df_sleep = df[df["event_name"].str.lower() == "sleep"].copy()
    df_nap = df[df["event_name"].str.lower() == "nap"].copy()

    # Sum durations per day.
    sleep_daily = df_sleep.groupby("date")["duration"].sum()
    nap_daily = df_nap.groupby("date")["duration"].sum()

    # Create a full daily index spanning the data range.
    start_date = min(
        sleep_daily.index.min() if not sleep_daily.empty else df["date"].min(),
        nap_daily.index.min() if not nap_daily.empty else df["date"].min(),
    )
    end_date = max(
        sleep_daily.index.max() if not sleep_daily.empty else df["date"].max(),
        nap_daily.index.max() if not nap_daily.empty else df["date"].max(),
    )
    full_index = pd.date_range(start=start_date, end=end_date, freq="D")

    # Reindex to include missing days (filling with zeros).
    sleep_daily = sleep_daily.reindex(full_index, fill_value=0)
    nap_daily = nap_daily.reindex(full_index, fill_value=0)

    # Combine into one DataFrame.
    daily = pd.DataFrame({"sleep_time": sleep_daily, "nap_time": nap_daily}, index=full_index)

    # Compute averaged data if needed.
    if average_for > 1:
        daily_avg = daily.resample(f"{average_for}D").mean()
        # Reset index so that 'date' is a column.
        daily_avg = daily_avg.reset_index().rename(columns={"index": "date"})
    else:
        daily_avg = daily.reset_index().rename(columns={"index": "date"})

    plt.figure(figsize=(10, 5))

    if average_for > 1:
        # Plot stacked bars using the averaged data.
        bar_width = average_for  # width in days for the bar (no gap between periods)
        plt.bar(
            daily_avg["date"],
            daily_avg["sleep_time"],
            width=bar_width,
            align="edge",
            label="Sleep (avg)",
        )
        plt.bar(
            daily_avg["date"],
            daily_avg["nap_time"],
            bottom=daily_avg["sleep_time"],
            width=bar_width,
            align="edge",
            label="Nap (avg)",
        )
        # Plot original daily datapoints in the background as small, translucent markers.
        plt.scatter(
            daily.index, daily["sleep_time"], s=3, color="blue", alpha=0.2, label="Sleep (daily)"
        )
        plt.scatter(
            daily.index, daily["nap_time"], s=3, color="orange", alpha=0.2, label="Nap (daily)"
        )
    else:
        # For daily plotting (no averaging), plot just the stacked bars.
        bar_width = 1
        plt.bar(
            daily_avg["date"], daily_avg["sleep_time"], width=bar_width, align="edge", label="Sleep"
        )
        plt.bar(
            daily_avg["date"],
            daily_avg["nap_time"],
            bottom=daily_avg["sleep_time"],
            width=bar_width,
            align="edge",
            label="Nap",
        )

    plt.title(
        "Daily Sleep & Nap Durations (averaged over"
        f" {average_for} day{'s' if average_for>1 else ''})"
    )
    plt.xlabel("Date")
    plt.ylabel("Hours")
    plt.ylim((4, 9.5))
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

    daily_avg["total_rest"] = daily_avg["sleep_time"] + daily_avg["nap_time"]
    return daily_avg


####                               #### Graph of all time combined
def bar_calendar_time(df, average_for=1):
    """
    Produces a stacked bar plot of total time per calendar with a continuous, evenly spaced x-axis.
    Bars are aggregated by day (or averaged over nonoverlapping groups of days if average_for > 1)
    and are plotted with no gaps. Uses a fixed color mapping for specific calendar names:
      - "Things": Grey
      - "Meals, Supplements, Sleep": Green
      - "Waste Time": Red
      - "clark.benham@gmail.com": Blue
      - "cb5ye@virginia.edu": Blue
    """
    # Fixed color mapping.
    # Ensure each row has a proper 'date'
    df2 = df.copy()
    df2["date"] = pd.to_datetime(df2["start_time"].dt.date)

    # Group by date and calendar_name: sum durations (in hours)
    grouped = df2.groupby(["date", "calendar_name"])["duration"].sum().unstack(fill_value=0)

    # Reindex to a full daily date range (so days with no events appear as zeros)
    full_index = pd.date_range(start=grouped.index.min(), end=grouped.index.max(), freq="D")
    grouped = grouped.reindex(full_index, fill_value=0)
    grouped.index.name = "date"

    # Average over nonoverlapping windows if requested.
    if average_for > 1:
        grouped = grouped.resample(f"{average_for}D").mean()

    # Convert the index (dates) to matplotlib numeric dates.
    dates = mdates.date2num(grouped.index.to_pydatetime())

    # Prepare for a stacked bar plot.
    categories = grouped.columns
    bottom = np.zeros(len(grouped))

    plt.figure(figsize=(10, 5))
    # Set bar width equal to the averaging period (in days) to ensure no gaps.
    width = average_for

    for cat in categories:
        values = grouped[cat].values
        # Get color from mapping; if not found, default to grey.
        col = color_map.get(cat, "grey")
        plt.bar(dates, values, bottom=bottom, width=width, align="edge", color=col, label=cat)
        bottom += values

    ax = plt.gca()
    ax.xaxis_date()
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
    plt.xticks(rotation=45)
    plt.title(
        f"Total Time per Calendar (averaged over {average_for} day{'s' if average_for>1 else ''})"
    )
    plt.xlabel("Date")
    plt.ylabel("Average Time (hours)")
    plt.legend()
    plt.tight_layout()
    plt.show()

    return grouped


def graph_activity_breakdown(df, average_for=1):
    """
    Produces a stacked bar plot showing, for each day (or an average over nonoverlapping windows of days),
    how much time is spent on each activity (by event_name). Any remaining time out of 24 hours is labeled
    as 'uncategorized'. Event names are truncated to 10 characters, ensuring uniqueness.

    Colors are assigned based on calendar_name.
    Uses a fixed color mapping:
      - "Things"                   -> Grey
      - "Meals, Supplements, Sleep"-> Green
      - "Waste Time"               -> Red
      - "clark.benham@gmail.com"   -> Blue
      - "cb5ye@virginia.edu"       -> Blue

    Bars are sorted by duration and plotted with no gaps. Text is added directly on each
    segment if the average is >=0.5 hours (30 minutes).
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    # Fixed color mapping for calendar names
    color_map = {
        "Things": "grey",
        "Meals, Supplements, Sleep": "green",
        "Waste Time": "red",
        "clark.benham@gmail.com": "blue",
        "cb5ye@virginia.edu": "blue",
    }

    # Prepare data for truncation and lowercase conversion
    df2 = df.copy()
    df2["date"] = pd.to_datetime(df2["start_time"].dt.date)

    # Create a combined column for grouping - convert event_name to lowercase
    df2["event_cal"] = df2["event_name"].str.lower() + " (" + df2["calendar_name"] + ")"

    # Group by date and combined event_cal: sum durations (in hours)
    grouped = df2.groupby(["date", "event_cal"])["duration"].sum().unstack(fill_value=0)

    # Add 'uncategorized' time: remaining hours in the day
    grouped["uncategorized"] = (24 - grouped.sum(axis=1)).clip(lower=0)

    # Reindex over the full daily date range
    full_index = pd.date_range(start=grouped.index.min(), end=grouped.index.max(), freq="D")
    grouped = grouped.reindex(full_index, fill_value=0)
    grouped.index.name = "date"

    # Average over nonoverlapping windows if needed
    if average_for > 1:
        grouped = grouped.resample(f"{average_for}D").mean()

    daily = grouped.reset_index()

    # Create mapping between event_cal columns and their calendar names
    event_cal_to_calendar = {}
    for event_cal in [col for col in daily.columns if col != "date" and col != "uncategorized"]:
        # Extract calendar name from the combined string (format: "event_name (calendar_name)")
        try:
            calendar_name = event_cal.split("(")[1].rstrip(")")
            event_cal_to_calendar[event_cal] = calendar_name
        except:
            # Fallback for any columns that don't match the expected format
            event_cal_to_calendar[event_cal] = "unknown"

    # Set calendar for uncategorized
    event_cal_to_calendar["uncategorized"] = "uncategorized"

    # Create unique truncated column names for all columns except 'date'
    seen = {}

    def unique_truncate(col, max_len=15):
        base = col[:max_len]
        if base in seen:
            seen[base] += 1
            return f"{base}_{seen[base]}"
        else:
            seen[base] = 1
            return base

    # Build mapping from original column to unique truncated name
    trunc_map = {}
    for col in daily.columns:
        if col == "date":
            trunc_map[col] = col
        else:
            # Get the event name part without the calendar part
            if col != "uncategorized":
                try:
                    event_name = col.split(" (")[0]
                    trunc = event_name[:15]  # Truncate to 15 chars
                except:
                    trunc = col[:10]
            else:
                trunc = col

            # Ensure uniqueness
            trunc = unique_truncate(trunc)
            trunc_map[col] = trunc

    # Rename columns
    daily_trunc = daily.rename(columns=trunc_map)

    # Create a reverse mapping from truncated name back to original
    # In case of multiple originals mapping to the same truncated name,
    # we'll just take the first one for color assignment purposes
    trunc_to_orig = {}
    for orig, trunc in trunc_map.items():
        if trunc not in trunc_to_orig and orig != "date":
            trunc_to_orig[trunc] = orig

    # Calculate total duration for each event across all days (for sorting)
    event_totals = daily.drop(columns=["date"]).sum().sort_values(ascending=False)

    # Sort columns by total duration (excluding date and keeping uncategorized at the end)
    sorted_events = event_totals.index.tolist()
    if "uncategorized" in sorted_events:
        sorted_events.remove("uncategorized")
        sorted_events.append("uncategorized")

    # Get truncated column names in sorted order
    sorted_trunc_events = [trunc_map[ev] for ev in sorted_events if ev in trunc_map]

    # Build a mapping for colors based on calendar name
    trunc_color = {}
    for trunc, orig in trunc_to_orig.items():
        if orig == "date":
            continue

        calendar = event_cal_to_calendar.get(orig, "unknown")

        # Assign color based on calendar name
        assigned_color = "grey"  # Default color
        for key, color in color_map.items():
            if key.lower() in calendar.lower():
                assigned_color = color
                break

        trunc_color[trunc] = assigned_color

    # Special color for uncategorized
    if "uncategorized" in trunc_map.values():
        trunc_color["uncategorized"] = "lightgrey"

    # Plot manually using plt.bar to control spacing
    # Dynamic figure width based on number of rows: 2 + 3 * number_of_rows
    num_rows = len(daily_trunc)
    fig_width = 2 + 3 * num_rows
    fig, ax = plt.subplots(figsize=(fig_width, 15))

    # Fix the x-axis positioning to ensure even spacing
    x_positions = np.arange(len(daily_trunc))
    bar_width = 0.9  # Width to accommodate 15 characters

    # For each day/time chunk, sort the events by their duration within that specific chunk
    for i, (_, row) in enumerate(daily_trunc.iterrows()):
        day_data = row.drop("date").to_dict()
        # Sort events by duration for this specific day/chunk
        day_sorted_events = sorted(
            [(event, duration) for event, duration in day_data.items()],
            key=lambda x: x[1],
            reverse=False,  # Sort ascending so largest are on top of stack
        )

        bottom = 0
        for event, height in day_sorted_events:
            if height > 0:  # Only plot if there's any duration
                color = trunc_color.get(event, "grey")
                bar = ax.bar(
                    x_positions[i],
                    height,
                    bottom=bottom,
                    width=bar_width,
                    align="center",
                    color=color,
                )

                # Add text label if duration is >= 0.1667 hours (10 minutes)
                if height >= 0.1667:
                    ax.text(
                        x_positions[i],
                        bottom + height / 2,
                        event,
                        ha="center",
                        va="center",
                        color="white",
                        fontsize=8,
                    )

                bottom += height

    # Set x-axis labels as the dates with even spacing
    ax.set_xticks(x_positions)
    ax.set_xticklabels(daily_trunc["date"].dt.strftime("%Y-%m-%d"), rotation=45)
    ax.set_xlabel("Date")
    ax.set_ylabel("Average Time (hours)")
    ax.set_title(
        f"Daily Activity Breakdown (averaged over {average_for} day{'s' if average_for>1 else ''})"
    )

    # Create legend: group by calendar name and only include those with events >= 0.5 hours
    legend_items = {}

    # Calculate average duration by event
    event_averages = daily_trunc.drop(columns=["date"]).mean()

    # Group legend items by calendar and only include those with sufficient average time
    for trunc, orig in trunc_to_orig.items():
        if trunc == "date":
            continue

        # Skip events with average less than 10 minutes
        if event_averages.get(trunc, 0) < 0.1667:
            continue

        calendar = event_cal_to_calendar.get(orig, "unknown")
        color = trunc_color.get(trunc, "grey")

        if calendar not in legend_items:
            legend_items[calendar] = (color, [])

        legend_items[calendar][1].append(trunc)

    # Build the legend
    legend_handles = []
    legend_labels = []

    for calendar, (color, events) in sorted(legend_items.items()):
        legend_handles.append(plt.Rectangle((0, 0), 1, 1, color=color))
        if len(events) <= 3:
            # For few events, list them all
            legend_labels.append(f"{calendar}: {', '.join(events)}")
        else:
            # For many events, just show calendar name
            legend_labels.append(calendar)

    if legend_handles:
        ax.legend(legend_handles, legend_labels, loc="upper right")

    plt.tight_layout()
    plt.show()

    return daily_trunc


# Extract data from work tracking csv
def load_work_summary(csv_file):
    """
    Loads the work summary CSV (which has extra header rows) and extracts only the
    columns 'Start Date', 'Value', and 'Hours Working'. It converts the date and numeric
    columns appropriately, and computes the work productivity as Value * Hours Working.
    """
    print("WARN: this doesn't account for the plug entries at the end of the week")
    # Read CSV while skipping the second row (which contains extra header info).
    # Adjust skiprows if your file structure differs.
    df = pd.read_csv(csv_file, skiprows=[1])

    # Select the needed columns.
    # These must match the CSV column names exactly.
    df = df[["Start Date", "Value", "Hours Working"]].copy()

    # Convert Start Date to datetime.
    df["date"] = pd.to_datetime(df["Start Date"], errors="coerce")

    # Convert Value and Hours Working to numeric.
    df["Value"] = pd.to_numeric(df["Value"], errors="coerce")
    df["Hours Working"] = pd.to_numeric(df["Hours Working"], errors="coerce")

    # Compute the productivity metric.
    df["work_productivity"] = df["Value"] * df["Hours Working"]

    # Drop rows with missing date or productivity.
    df = df.dropna(subset=["date", "work_productivity"])

    # Combinte rows on the same date, eg both a personal and hive work
    df = df.groupby("date", as_index=False).agg(
        {"Hours Working": "sum", "work_productivity": "sum", "Value": "mean"}
    )
    return df


if __name__ == "__main__":
    # Run tests.
    # run_tests()

    # Example usage (comment out tests if processing real calendar data)
    # calendar_dir = "data/Calendar Takeout/Calendar/"
    # calendar_dir = "/Users/clarkbenham/Downloads/calendar exports 05_08_25"
    # calendar_dir = "/Users/clarkbenham/Downloads/Takeout 3/Calendar"
    calendar_dir = "/Users/clarkbenham/Downloads/Takeout 5 11_08_2025/Calendar/"
    start_date = datetime(2021, 5, 24)
    end_date = datetime(2025, 5, 7)
    end_date = datetime(2025, 11, 7)
    today_date = datetime.combine(datetime.today(), datetime.max.time())
    if end_date != today_date:
        print(f"INFO: Date cutoff  {end_date} is before today {today_date}")
    df = parse_ics_files(calendar_dir, start_date, end_date)
    _df = df.copy()
    # r = debug_sleep_patterns(df)
    # Rename calendar columns to remove extra identifiers
    df["calendar_name"] = df["calendar_name"].replace(
        {
            "Waste Time_d96ngv72j0b35hggvsjj1i8rf8@group.calendar.google.com": "Waste Time",
            "Meals, Supplements, Sleep_i740223r4sepqshulbhc8qques@group.calendar.google.com": (
                "Meals, Supplements, Sleep"
            ),
            "Things_f8243s41ks1occsl85o25hrf24@group.calendar.google.com": "Things",
        }
    )

    df = process_sleep_events(df)
    df = process_slash_events(df)
    df = process_book_events(df)
    df = process_overlaps(df)
    # # count work as productive time; work: hive or job: hive
    df.loc[df["event_name"].str.lower().str.contains(": hive"), "calendar_name"] = (
        "clark.benham@gmail.com"
    )
    df.loc[df["event_name"].str.lower() == "job", "calendar_name"] = "clark.benham@gmail.com"
    # # reminders don't count

    # Print overall time summary.
    overall_time_summary(df, start_date, end_date)
    if False:  # graph of overall, just look at tabl instead
        agg = bar_calendar_time(df, average_for=7)
        plt.scatter(agg.index, agg["Waste Time"], color="red")
        plt.title("avg Waste Time/day over Week")
        plt.show()

        # start_date = pd.to_datetime("2025-01-01", utc=True)
        # filtered_df = df.query("start_time >= @start_date")
        filtered_df = df
        graph_activity_breakdown(filtered_df, average_for=28)
    work_data = load_work_summary("data/Work Summary  - Daily Summary.csv")
    # df.to_csv("data/calendar_analysis.csv", index=True)
    df.drop(columns="metadata").to_csv("data/calendar_analysis.txt", sep="\t", index=False)
    work_data.to_csv("data/work_analysis.txt", sep="\t", index=False)
    # %%
    # 2. Top-N events per week and month.
    weekly_top_events(df, start_date, end_date, top_n=10)
    monthly_top_events(df, start_date, end_date, top_n=10)

    # Graphs
    scatter_calendar_time(df, average_for=14)
    graph_drink_count(df)
    graph_waste_days(df)

    graph_sleep_nap(df, average_for=7)
    graph_sleep_nap(df, average_for=28)
    graph_sleep_nap(df, average_for=120)

    graph_gym_count(df, average_for=7)
    gym_data = graph_gym_count(df, average_for=28)
    graph_gym_count(df, average_for=100)

    # Most and least ever worked out and slept in a month
    # For Gym – using graph_gym_count
    most_worked_out = gym_data.loc[gym_data["count"].idxmax()]
    least_worked_out = gym_data.loc[gym_data["count"].idxmin()]

    print("Most worked out month (28-day average):")
    print(most_worked_out)
    print("Least worked out month (28-day average):")
    print(least_worked_out)

    # For Sleep – using graph_sleep_nap
    sleep_data = graph_sleep_nap(df, average_for=28)
    most_slept = sleep_data.loc[sleep_data["total_rest"].idxmax()]
    least_slept = sleep_data.loc[sleep_data["total_rest"].idxmin()]

    print("Most slept month (28-day average) – based on total rest:")
    print(most_slept)
    print("Least slept month (28-day average) – based on total rest:")
    print(least_slept)

    graph_activity_breakdown(df, average_for=28)

# %%
# Graphs including info from work tracking csv
# Analysis of sleep: does napping reduce? How does impact hours worked?


def scatter_total_rest_vs_nap(sleep_data):
    """
    Given sleep_data DataFrame (with columns: date, sleep_time, nap_time),
    compute total rest = sleep_time + nap_time, then scatter-plot total rest (y)
    versus nap_time (x). A linear regression line is added along with its slope,
    intercept, and R² value.
    """
    # Compute total rest.
    sleep_data = sleep_data.copy()
    sleep_data["total_rest"] = sleep_data["sleep_time"] + sleep_data["nap_time"]

    # Define x and y for regression.
    x = sleep_data["nap_time"].values
    y = sleep_data["total_rest"].values

    plt.figure(figsize=(8, 6))
    plt.scatter(x, y, alpha=0.7, label="Data points")

    # Compute linear regression (degree 1 polynomial).
    coef = np.polyfit(x, y, 1)
    poly1d_fn = np.poly1d(coef)
    # Generate regression line using the sorted x values.
    x_sorted = np.sort(x)
    plt.plot(
        x_sorted, poly1d_fn(x_sorted), color="red", label=f"Fit: y={coef[0]:.2f}x+{coef[1]:.2f}"
    )

    # Compute R^2.
    y_pred = poly1d_fn(x)
    r2 = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)
    plt.text(
        0.05,
        0.95,
        f"R² = {r2:.2f}",
        transform=plt.gca().transAxes,
        verticalalignment="top",
        bbox=dict(facecolor="white", alpha=0.7),
    )

    plt.xlabel("Nap Time (hours)")
    plt.ylabel("Total Rest Time (hours)")
    plt.title("Scatter: Total Rest vs. Nap Time")
    plt.legend()
    plt.tight_layout()
    plt.show()

    return sleep_data


def scatter_total_rest_vs_clark(df, sleep_data):
    """
    Given the overall DataFrame (df) and sleep_data (with columns: date, sleep_time, nap_time),
    this function computes total rest as sleep_time + nap_time, then extracts the total duration
    per day for events from calendar 'clark.benham@gmail.com'. It merges the two by date and plots
    total rest (x) versus total clark event duration (y), along with a best-fit line and R².
    """
    # Compute total rest in sleep_data.
    sleep_data = sleep_data.copy()
    sleep_data["total_rest"] = sleep_data["sleep_time"] + sleep_data["nap_time"]

    # Get clark events.
    clark_df = df[df["calendar_name"] == "clark.benham@gmail.com"].copy()
    clark_df["date"] = pd.to_datetime(clark_df["start_time"].dt.date)
    # Group by date: sum durations.
    clark_daily = clark_df.groupby("date")["duration"].sum().reset_index()

    # Merge sleep_data and clark_daily on date.
    merged = pd.merge(sleep_data, clark_daily, on="date", how="inner")
    # Rename for clarity.
    merged = merged.rename(columns={"duration": "clark_duration"})

    # Define x and y for regression.
    x = merged["total_rest"].values
    y = merged["clark_duration"].values

    plt.figure(figsize=(8, 6))
    plt.scatter(x, y, alpha=0.7, label="Data points")

    # Compute regression.
    coef = np.polyfit(x, y, 1)
    poly1d_fn = np.poly1d(coef)
    x_sorted = np.sort(x)
    plt.plot(
        x_sorted, poly1d_fn(x_sorted), color="red", label=f"Fit: y={coef[0]:.2f}x+{coef[1]:.2f}"
    )

    # Compute R².
    y_pred = poly1d_fn(x)
    r2 = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)
    plt.text(
        0.05,
        0.95,
        f"R² = {r2:.2f}",
        transform=plt.gca().transAxes,
        verticalalignment="top",
        bbox=dict(facecolor="white", alpha=0.7),
    )

    plt.xlabel("Total Rest Time (hours)")
    plt.ylabel("Total 'clark.benham@gmail.com' Duration (hours)")
    plt.title("Scatter: Total Rest vs. 'clark.benham@gmail.com' Events")
    plt.legend()
    plt.tight_layout()
    plt.show()

    return merged


def scatter_sleep_vs_work_aggregated(sleep_data, work_data):
    """
    Aggregates work_data over periods defined by sleep_data dates.

    For each sleep_data entry, assume its period is from that date (inclusive)
    to the next sleep_data date. For the last sleep_data entry, use the period
    from that date to work_data['date'].max(), but assert the gap isn't longer
    than the maximum of the previous gaps.

    Then, compute for each period:
      - total_rest = sleep_time + nap_time (from sleep_data at the period start)
      - average work_productivity and average Hours Working (from work_data)

    Finally, plot two scatter plots:
      1. total_rest (x) vs. avg work_productivity (y)
      2. total_rest (x) vs. avg Hours Working (y)

    In both, the aggregated points are color-coded from red (oldest) to blue (newest)
    and a linear best-fit line is added (with slope, intercept, and R² shown).

    Returns a DataFrame with one row per aggregated period.
    """
    # Ensure dates are datetime and sort
    sleep_data = sleep_data.copy()
    sleep_data["date"] = pd.to_datetime(sleep_data["date"])
    sleep_data = sleep_data.sort_values("date").reset_index(drop=True)

    work_data = work_data.copy()
    work_data["date"] = pd.to_datetime(work_data["date"])
    work_data = work_data.sort_values("date").reset_index(drop=True)

    # Compute total_rest in sleep_data.
    sleep_data["total_rest"] = sleep_data["sleep_time"] + sleep_data["nap_time"]

    # Build intervals from sleep_data dates.
    sleep_dates = sleep_data["date"].tolist()
    intervals = []
    for i in range(len(sleep_dates) - 1):
        intervals.append((sleep_dates[i], sleep_dates[i + 1]))
    # For the last interval: from last sleep date to max date in work_data.
    last_interval = (sleep_dates[-1], work_data["date"].max())
    # Check gap lengths.
    previous_gaps = [
        (sleep_dates[i + 1] - sleep_dates[i]).days for i in range(len(sleep_dates) - 1)
    ]
    max_gap = max(previous_gaps) if previous_gaps else 0
    last_gap = (last_interval[1] - last_interval[0]).days
    if max_gap > 0 and last_gap > max_gap:
        raise ValueError(
            f"Last interval gap ({last_gap} days) exceeds maximum previous gap ({max_gap} days)."
        )
    intervals.append(last_interval)

    # For each interval, compute average work_productivity and average Hours Working.
    agg_list = []
    for start, end in intervals:
        mask = (work_data["date"] >= start) & (work_data["date"] < end)
        sub = work_data.loc[mask]
        if not sub.empty:
            avg_prod = sub["work_productivity"].mean()
            avg_hours = sub["Hours Working"].mean()  # assumes work_data has this column
        else:
            avg_prod = np.nan
            avg_hours = np.nan
        # Get sleep metrics from the sleep_data row at the start date.
        sleep_row = sleep_data[sleep_data["date"] == start].iloc[0]
        agg_list.append(
            {
                "date": start,
                "total_rest": sleep_row["total_rest"],
                "sleep_time": sleep_row["sleep_time"],
                "nap_time": sleep_row["nap_time"],
                "avg_work_productivity": avg_prod,
                "avg_hours_working": avg_hours,
            }
        )
    agg_df = pd.DataFrame(agg_list).dropna()

    # Build a colormap: map date timestamps to a value from 0 (old) to 1 (new)
    norm = plt.Normalize(agg_df["date"].min().timestamp(), agg_df["date"].max().timestamp())
    cmap = plt.get_cmap("RdBu")  # red (old) to blue (new)
    colors = [cmap(norm(d.timestamp())) for d in agg_df["date"]]

    # Create subplots: one for productivity and one for hours working.
    fig, axs = plt.subplots(1, 2, figsize=(14, 6))

    # Plot 1: total_rest vs. avg_work_productivity
    x = agg_df["total_rest"].values
    y1 = agg_df["avg_work_productivity"].values
    axs[0].scatter(x, y1, color=colors, s=80, alpha=0.8, label="Data")
    # Linear regression.
    coef1 = np.polyfit(x, y1, 1)
    poly1 = np.poly1d(coef1)
    x_sorted = np.sort(x)
    axs[0].plot(
        x_sorted,
        poly1(x_sorted),
        color="black",
        lw=2,
        label=f"Fit: y={coef1[0]:.2f}x+{coef1[1]:.2f}",
    )
    y1_pred = poly1(x)
    r2_1 = 1 - np.sum((y1 - y1_pred) ** 2) / np.sum((y1 - np.mean(y1)) ** 2)
    axs[0].text(
        0.05,
        0.95,
        f"R² = {r2_1:.2f}",
        transform=axs[0].transAxes,
        verticalalignment="top",
        bbox=dict(facecolor="white", alpha=0.7),
    )
    axs[0].set_xlabel("Total Rest (hours)")
    axs[0].set_ylabel("Avg Work Productivity")
    axs[0].set_title("Sleep vs. Work Productivity")
    axs[0].legend()

    # Plot 2: total_rest vs. avg_hours_working
    y2 = agg_df["avg_hours_working"].values
    axs[1].scatter(x, y2, color=colors, s=80, alpha=0.8, label="Data")
    coef2 = np.polyfit(x, y2, 1)
    poly2 = np.poly1d(coef2)
    axs[1].plot(
        x_sorted,
        poly2(x_sorted),
        color="black",
        lw=2,
        label=f"Fit: y={coef2[0]:.2f}x+{coef2[1]:.2f}",
    )
    y2_pred = poly2(x)
    r2_2 = 1 - np.sum((y2 - y2_pred) ** 2) / np.sum((y2 - np.mean(y2)) ** 2)
    axs[1].text(
        0.05,
        0.95,
        f"R² = {r2_2:.2f}",
        transform=axs[1].transAxes,
        verticalalignment="top",
        bbox=dict(facecolor="white", alpha=0.7),
    )
    axs[1].set_xlabel("Total Rest (hours)")
    axs[1].set_ylabel("Avg Hours Working")
    axs[1].set_title("Sleep vs. Hours Working")
    axs[1].legend()

    # Create a colorbar showing the date scale.
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    cbar = fig.colorbar(sm, ax=axs, orientation="vertical", fraction=0.05, pad=0.02)
    cbar.set_label("Date (red=old, blue=new)")

    plt.tight_layout()
    plt.show()

    return agg_df


if __name__ == "__main__":
    sleep_data = graph_sleep_nap(df, average_for=14)
    scatter_total_rest_vs_nap(sleep_data)
    scatter_total_rest_vs_clark(df, sleep_data)

    merged_data = scatter_sleep_vs_work_aggregated(sleep_data, work_data)

    # Only from tue or wed start of aggr
    weekday_start = merged_data  # [merged_data["date"].dt.dayofweek.isin([1, 2])]
    for c1 in ["avg_work_productivity", "avg_hours_working"]:
        for c2 in ["total_rest", "sleep_time"]:
            plt.scatter(weekday_start[c2], weekday_start[c1])
            plt.title(f"{c1}- {c2}")
            plt.xlabel(c2)
            plt.ylabel(c1)
            plt.show()


# %% Impact of workouts on job performance
def scatter_workout_vs_work_aggregated(workout_data, work_data):
    """
    Aggregates workout_data over periods defined by its 'date' entries.
    For each period (from one workout date to the next), it computes:
      - Average workout_count and workout_hours from workout_data
      - Average work_productivity and average Hours Working from work_data
    The last period runs from the last workout date to work_data['date'].max() – an assertion
    is raised if that final gap is longer than any previous gap.

    Then, two scatter plots are produced:
      (1) Average workout count vs. average work productivity.
      (2) Average workout hours vs. average Hours Working.

    Data points are color-coded from red (old) to blue (new) according to their start date.
    Regression lines (with slope, intercept, and R²) are added to each plot.

    Returns:
        A DataFrame with one row per aggregated interval.
    """
    # Ensure the date columns are datetime and sort both DataFrames.
    workout_data = workout_data.copy()
    workout_data["date"] = pd.to_datetime(workout_data["date"])
    workout_data = workout_data.sort_values("date").reset_index(drop=True)

    work_data = work_data.copy()
    work_data["date"] = pd.to_datetime(work_data["date"])
    work_data = work_data.sort_values("date").reset_index(drop=True)

    # Build intervals using workout_data's dates.
    workout_dates = workout_data["date"].tolist()
    intervals = []
    for i in range(len(workout_dates) - 1):
        intervals.append((workout_dates[i], workout_dates[i + 1]))
    # For the final interval: from last workout date to work_data's max date.
    last_interval = (workout_dates[-1], work_data["date"].max())
    previous_gaps = [
        (workout_dates[i + 1] - workout_dates[i]).days for i in range(len(workout_dates) - 1)
    ]
    max_gap = max(previous_gaps) if previous_gaps else 0
    last_gap = (last_interval[1] - last_interval[0]).days
    if max_gap > 0 and last_gap > max_gap:
        raise ValueError(
            f"Last interval gap ({last_gap} days) exceeds maximum previous gap ({max_gap} days)."
        )
    intervals.append(last_interval)

    # Aggregate data for each interval.
    agg_list = []
    for start, end in intervals:
        # Aggregate workout_data for this period.
        mask_w = (workout_data["date"] >= start) & (workout_data["date"] < end)
        sub_w = workout_data.loc[mask_w]
        if not sub_w.empty:
            avg_workout_count = sub_w["count"].mean()
            avg_workout_hours = sub_w["hours"].mean()
        else:
            avg_workout_count = np.nan
            avg_workout_hours = np.nan

        # Aggregate work_data for this period.
        mask_work = (work_data["date"] >= start) & (work_data["date"] < end)
        sub_work = work_data.loc[mask_work]
        if not sub_work.empty:
            avg_work_productivity = sub_work["work_productivity"].mean()
            avg_hours_working = sub_work["Hours Working"].mean()
        else:
            avg_work_productivity = np.nan
            avg_hours_working = np.nan

        agg_list.append(
            {
                "date": start,
                "avg_workout_count": avg_workout_count,
                "avg_workout_hours": avg_workout_hours,
                "avg_work_productivity": avg_work_productivity,
                "avg_hours_working": avg_hours_working,
            }
        )
    agg_df = pd.DataFrame(agg_list).dropna()

    # Color coding: map each aggregated date to a color (red=old, blue=new).
    norm = plt.Normalize(agg_df["date"].min().timestamp(), agg_df["date"].max().timestamp())
    cmap = plt.get_cmap("RdBu")
    colors = [cmap(norm(d.timestamp())) for d in agg_df["date"]]

    fig, axs = plt.subplots(1, 2, figsize=(14, 6))

    # Plot 1: avg_workout_count vs. avg_work_productivity.
    x1 = agg_df["avg_workout_count"].values
    y1 = agg_df["avg_work_productivity"].values
    axs[0].scatter(x1, y1, color=colors, s=80, alpha=0.8, label="Data")
    coef1 = np.polyfit(x1, y1, 1)
    poly1 = np.poly1d(coef1)
    x1_sorted = np.sort(x1)
    axs[0].plot(
        x1_sorted,
        poly1(x1_sorted),
        color="black",
        lw=2,
        label=f"Fit: y={coef1[0]:.2f}x+{coef1[1]:.2f}",
    )
    y1_pred = poly1(x1)
    r2_1 = 1 - np.sum((y1 - y1_pred) ** 2) / np.sum((y1 - np.mean(y1)) ** 2)
    axs[0].text(
        0.05,
        0.95,
        f"R² = {r2_1:.2f}",
        transform=axs[0].transAxes,
        verticalalignment="top",
        bbox=dict(facecolor="white", alpha=0.7),
    )
    axs[0].set_xlabel("Avg Workout Count")
    axs[0].set_ylabel("Avg Work Productivity")
    axs[0].set_title("Workout Count vs. Work Productivity")
    axs[0].legend()

    # Plot 2: avg_workout_hours vs. avg_hours_working.
    x2 = agg_df["avg_workout_hours"].values
    y2 = agg_df["avg_hours_working"].values
    axs[1].scatter(x2, y2, color=colors, s=80, alpha=0.8, label="Data")
    coef2 = np.polyfit(x2, y2, 1)
    poly2 = np.poly1d(coef2)
    x2_sorted = np.sort(x2)
    axs[1].plot(
        x2_sorted,
        poly2(x2_sorted),
        color="black",
        lw=2,
        label=f"Fit: y={coef2[0]:.2f}x+{coef2[1]:.2f}",
    )
    y2_pred = poly2(x2)
    r2_2 = 1 - np.sum((y2 - y2_pred) ** 2) / np.sum((y2 - np.mean(y2)) ** 2)
    axs[1].text(
        0.05,
        0.95,
        f"R² = {r2_2:.2f}",
        transform=axs[1].transAxes,
        verticalalignment="top",
        bbox=dict(facecolor="white", alpha=0.7),
    )
    axs[1].set_xlabel("Avg Workout Hours")
    axs[1].set_ylabel("Avg Hours Working")
    axs[1].set_title("Workout Hours vs. Hours Working")
    axs[1].legend()

    # Add a colorbar for the date scale.
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    cbar = fig.colorbar(sm, ax=axs, orientation="vertical", fraction=0.05, pad=0.02)
    cbar.set_label("Date (red=old, blue=new)")

    plt.tight_layout()
    plt.show()

    return agg_df


if __name__ == "__main__":
    # not "correct" way to aggr by weekday, but close enough
    gym_data = graph_gym_count(df, average_for=2)
    agg_workout = scatter_workout_vs_work_aggregated(gym_data, work_data)

    # weekday_start = agg_workout[agg_workout["date"].dt.dayofweek.isin([1, 2])]
    weekday_start = agg_workout[agg_workout["date"].dt.dayofweek.isin([0])]
    norm = plt.Normalize(
        weekday_start["date"].min().timestamp(), weekday_start["date"].max().timestamp()
    )
    cmap = plt.get_cmap("RdBu")
    colors = [cmap(norm(d.timestamp())) for d in weekday_start["date"]]
    for c1 in ["avg_work_productivity", "avg_hours_working"]:
        for c2 in ["avg_workout_count", "avg_workout_hours"]:
            plt.scatter(weekday_start[c2], weekday_start[c1], color=colors)
            plt.title(f"{c1}- {c2}")
            plt.show()

# %% Regression prediction
import pandas as pd
import numpy as np
import statsmodels.api as sm
from datetime import timedelta


def partition_calendar_by_sleep(df, phrases):
    """
    Partition the calendar data by sleep times and extract features.
    """
    df = df.sort_values("start_time").reset_index(drop=True)

    # Create a mask for sleep events
    sleep_mask = df["event_name"].str.lower() == "sleep"

    # If no sleep events found, return an empty dataframe with expected columns
    if not sleep_mask.any():
        print("No sleep events found in calendar data")
        col_list = ["date"] + [f"{p}_count" for p in phrases] + ["book_over_1", "sleep_hours"]
        return pd.DataFrame(columns=col_list)

    # Get indices of sleep events
    sleep_indices = df.loc[sleep_mask].index.tolist()
    print(f"Processing {len(sleep_indices)} sleep events")

    # Create partitions between consecutive sleep events
    partitions = []
    for i, idx in enumerate(sleep_indices):
        # Use the date of the sleep event as the partition date
        sleep_date = df.loc[idx, "start_time"].date()

        # If this is the last sleep event, use all remaining data
        if i == len(sleep_indices) - 1:
            next_start = df["start_time"].max() + pd.Timedelta(seconds=1)
        else:
            next_start = df.loc[sleep_indices[i + 1], "start_time"]

        # Get all events in this partition (including the sleep event itself)
        start_time = df.loc[idx, "start_time"]
        block = df[(df["start_time"] >= start_time) & (df["start_time"] < next_start)]

        partitions.append({"date": sleep_date, "block": block})

    # Extract features from each partition
    rows = []
    for part in partitions:
        events = part["block"]
        d = part["date"]
        row = {"date": d}

        # Add weekend indicator
        row["is_weekend"] = 1 if pd.to_datetime(d).weekday() >= 5 else 0

        # Count occurrences of each phrase
        for phrase in phrases:
            row[f"{phrase}_count"] = int(  # actually has
                events["event_name"].str.lower().str.contains(phrase.lower(), na=False).sum() > 0
            )

        # Check for book events over 1 hour
        mask_book = events["event_name"].str.lower().str.contains("book:|read:", na=False)
        book_duration = events.loc[mask_book, "duration"].sum() if mask_book.any() else 0
        row["book_over_1"] = 1 if book_duration > 1 else 0

        # Get sleep hours
        mask_sleep = events["event_name"].str.lower() == "sleep"
        row["sleep_hours"] = events.loc[mask_sleep, "duration"].sum() if mask_sleep.any() else 0

        rows.append(row)

    result_df = pd.DataFrame(rows)
    print(f"Created {len(result_df)} day partitions from sleep events")
    return result_df


# Add to the imports at the top of your file
from scipy.optimize import minimize_scalar
import numpy as np


def add_optimal_period_feature(df, date_column="date", y_col="work_productivity"):
    """
    Find the optimal period for a sine wave feature and add it to the dataframe.
        (Didn't help with all other varian)
    Parameters:
    -----------
    df : DataFrame
        The dataframe containing the date column.
    date_column : str
        The name of the date column in the dataframe.

    Returns:
    --------
    DataFrame
        The dataframe with the added periodic features.
    float
        The optimal period found.
    """
    # Convert date to numeric (days since the earliest date)
    df = df.copy()
    df["day_number"] = (
        pd.to_datetime(df[date_column]) - pd.to_datetime(df[date_column]).min()
    ).dt.days

    y = df[y_col]

    # Function to evaluate period quality (negative R²)
    def evaluate_period(period):
        # Create sine and cosine features with the given period
        df["sin_term"] = np.cos(2 * np.pi * df["day_number"] / period)
        # df["cos_term"] = np.cos(2 * np.pi * df["day_number"] / period)

        # Simple linear regression with these features
        # X = sm.add_constant(df[["sin_term", "cos_term"]])
        X = sm.add_constant(df[["sin_term"]])
        model = sm.OLS(y, X).fit()

        # Return negative R² (for minimization)
        return -model.rsquared

    # Search for optimal period between 1 and 365 days
    result = minimize_scalar(evaluate_period, bounds=(1, 365), method="bounded")
    # result = minimize_scalar(evaluate_period, bounds=(1, 50), method="bounded")
    optimal_period = result.x

    print(f"Optimal period found: {optimal_period:.2f} days R^2={-result.fun}")

    # Add the final sine and cosine features with optimal period
    df["sin_term"] = np.sin(2 * np.pi * df["day_number"] / optimal_period)
    # df["cos_term"] = np.cos(2 * np.pi * df["day_number"] / optimal_period)

    return df, optimal_period


def average_over_percentile_range(X, y, l1=20, h1=40, l2=60, h2=80):
    # Calculate percentiles for y
    p_l1 = np.percentile(y, l1)
    p_h1 = np.percentile(y, h1)
    p_l2 = np.percentile(y, l2)
    p_h2 = np.percentile(y, h2)

    # Create masks for the two percentile ranges
    mask_1 = (y >= p_l1) & (y <= p_h1)
    mask_2 = (y >= p_l2) & (y <= p_h2)

    # Filter data for each range
    X_1 = X[mask_1]
    y_1 = y[mask_1]
    X_2 = X[mask_2]
    y_2 = y[mask_2]

    # Calculate means for each column in X and y
    means_1 = np.append(X_1.mean(axis=0), y_1.mean())
    means_2 = np.append(X_2.mean(axis=0), y_2.mean())

    # If X has column names (e.g., if it's a DataFrame), use them; otherwise use indices
    if hasattr(X, "columns"):
        column_names = list(X.columns) + ["y"]
    else:
        column_names = [f"X{i}" for i in range(X.shape[1])] + ["y"]

    # Print comparison
    print("Average values comparison:")
    print("-" * 90)
    print(
        f"{'Variable':<20} {f'{l1}-{h1}th %ile':<15} {f'{l2}-{h2}th %ile':<15} {'Difference':<15} {'% Change':<10}"
    )
    print("-" * 90)

    for i, col in enumerate(column_names):
        val_1 = means_1[i]
        val_2 = means_2[i]
        diff = val_2 - val_1

        if val_1 != 0:
            pct_change = ((val_2 - val_1) / abs(val_1)) * 100
            pct_str = f"{pct_change:.1f}%"
        else:
            pct_str = "N/A"

        print(f"{col:<20} {val_1:<15.3f} {val_2:<15.3f} {diff:<15.3f} {pct_str:<10}")


def regression_predict_work_from_sleep_breakpoints(
    calendar_df,
    work_df,
    phrase_list=["gym"],
    use_prev_day=True,
    min_weekly_hours=10,
):
    """
    Create a regression model to predict work productivity using calendar and work data.
    Uses sleep events to split days.

    Parameters:
    -----------
    calendar_df : DataFrame
        Processed calendar data with events.
    work_df : DataFrame
        Work data with productivity values.
    phrase_list : list of str
        List of phrases to count in calendar events.
    use_prev_day : bool
        Whether to include previous day's features.
    min_weekly_hours : float
        Minimum hours worked in a week to include in analysis.

    Returns:
    --------
    model : statsmodels.regression.linear_model.RegressionResultsWrapper
        The fitted regression model.
    merged : DataFrame
        The merged data used for training.
    """
    # Make copies to avoid modifying original data
    calendar_df = calendar_df.copy()
    work_df = work_df.copy()

    print(f"Calendar data shape: {calendar_df.shape}")
    print(f"Work data shape: {work_df.shape}")

    # First, ensure we have a date column that's properly normalized
    if "date" not in calendar_df.columns:
        calendar_df["date"] = pd.to_datetime(calendar_df["start_time"]).dt.date
    else:
        calendar_df["date"] = pd.to_datetime(calendar_df["date"]).dt.date

    # Normalize work_df dates to ensure proper merging
    work_df["date"] = pd.to_datetime(work_df["date"]).dt.date

    # Add yesterday's productivity to work_df
    work_df_with_prev = work_df.copy()
    date_to_productivity = dict(zip(work_df["date"], work_df["work_productivity"]))
    # for i in range(1, 25):
    for i in [1, 7]:
        work_df_with_prev[f"{i}_days_ago"] = work_df_with_prev["date"] - timedelta(days=i)
        work_df_with_prev[f"{i}_days_ago_prod"] = work_df_with_prev[f"{i}_days_ago"].map(
            date_to_productivity
        )
    # work_df_with_prev["yesterday_date"] = work_df_with_prev["date"] - timedelta(days=1)
    # work_df_with_prev["last_week_date"] = work_df_with_prev["date"] - timedelta(days=7)
    # # Create a mapping of date to productivity
    # date_to_productivity = dict(zip(work_df["date"], work_df["work_productivity"]))
    # # Add yesterday's productivity
    # work_df_with_prev["yesterday_productivity"] = work_df_with_prev["yesterday_date"].map(
    #     date_to_productivity
    # )
    # work_df_with_prev["last_week_productivity"] = work_df_with_prev["last_week_date"].map(
    #     date_to_productivity
    # )

    # date cutoffs features
    threshold_date = pd.Timestamp("2023-07-22").date()
    work_df_with_prev["after_hive"] = work_df_with_prev["date"].apply(
        lambda d: 1 if d >= threshold_date else 0
    )
    threshold_date = pd.Timestamp("2024-08-24").date()
    work_df_with_prev["after_mats"] = work_df_with_prev["date"].apply(
        lambda d: 1 if d >= threshold_date else 0
    )
    # Replace work_df with the enhanced version
    work_df = work_df_with_prev

    # Check for sleep events
    sleep_events = calendar_df[calendar_df["event_name"].str.lower() == "sleep"]
    print(f"Found {len(sleep_events)} sleep events in calendar")

    # Extract calendar features
    print("Extracting calendar features using sleep breakpoints...")
    cal_features = partition_calendar_by_sleep(calendar_df, phrase_list)
    print(cal_features.columns)

    # Include previous day's features if requested
    if use_prev_day and not cal_features.empty:
        print("Adding previous day features...")
        # Create a copy of features with dates shifted forward by 1 day
        prev = cal_features.copy()
        prev["date"] = prev["date"].apply(lambda d: (pd.to_datetime(d) + timedelta(days=1)).date())
        # Rename columns for previous day
        rename = {f"{p}_count": f"prev_{p}_count" for p in phrase_list}
        rename["book_over_1"] = "prev_book_over_1"  # Add this to avoid duplicate
        rename["sleep_hours"] = "prev_sleep_hours"
        # Don't include is_weekend in previous day features
        if "is_weekend" in prev.columns:
            prev = prev.drop(columns=["is_weekend"])
        prev = prev.rename(columns=rename)

        print(prev.columns)
        print(["date"] + list(rename.values()))
        # Merge with current day features
        cal_features = pd.merge(
            cal_features, prev[["date"] + list(rename.values())], on="date", how="left"
        )
        cal_features.fillna(0, inplace=True)

    # Merge calendar features with work data
    print(f"Calendar features shape: {cal_features.shape}, Work data shape: {work_df.shape}")
    merged = pd.merge(work_df, cal_features, on="date", how="inner")
    print(f"Merged data shape: {merged.shape}")

    # If merged data is empty, return early
    if merged.empty:
        print("ERROR: No matching dates between calendar features and work data")
        print("Date range in work data:", min(work_df["date"]), "to", max(work_df["date"]))
        if not cal_features.empty:
            print(
                "Date range in calendar features:",
                min(cal_features["date"]),
                "to",
                max(cal_features["date"]),
            )
        return

    # Add week column and filter by weekly hours threshold
    merged["week"] = pd.to_datetime(merged["date"]).dt.to_period("W")
    weekly_hours = merged.groupby("week")["Hours Working"].transform("sum")
    filtered = merged[(merged["work_productivity"] > 0) & (weekly_hours >= min_weekly_hours)]
    duplicate_columns = merged.columns[merged.T.duplicated()]
    print(f"Duplicate columns (keeping first): {list(duplicate_columns)}")
    filtered = filtered.loc[:, ~filtered.columns.duplicated(keep="first")]

    if filtered.empty:
        print(
            f"ERROR: No data points left after filtering for weeks with {min_weekly_hours}+ hours"
        )
        return None, merged
    print(f"Filtered data shape: {filtered.shape}")
    print(filtered.columns)

    # # add periodic column term
    # filtered, optimal_period = add_optimal_period_feature(filtered)
    # print(f"Added periodic features with period of {optimal_period:.2f} days")

    # Create model
    X = filtered.select_dtypes(include=np.number)  # [features].copy()
    # oringally had abs(t) <=0.9 on full daily work hours regression. But some I'll just keep in
    X = X.drop(
        columns=[
            "prev_sleep_hours",
            # "after_hive",
            # "insta_count",
            # "jack_count",
            "prev_gym_count",
            "prev_chores_count",
            "prev_book_count",
            "prev_twitter_count",
            "prev_insta_count",
            "prev_porn_count",
        ]
    )
    print(X.columns)
    print("Col not in regression", set(filtered.columns) - set(X.columns))

    # nan_rows = X[X.isna().any(axis=1)]
    # print("Rows with NaN values:")
    # print(nan_rows)
    # Replace NaN values with the mean of each column, eg start of previous days
    X = X.fillna(X.mean())

    pred_cols = ["Hours Working", "work_productivity"]
    X = X.drop(columns=pred_cols + ["Value"])

    print_var = False
    if print_var:
        numeric_df = X.select_dtypes(include=np.number)
        corr_matrix = numeric_df.corr()
        print(corr_matrix)
        epsilon = 1e-10
        regularized_corr = corr_matrix + np.eye(corr_matrix.shape[0]) * epsilon
        inverse_corr = np.linalg.inv(regularized_corr)
        print(
            "var unexplained by other entries: ",
            [(c, f"{1/i:.2f}") for c, i in zip(regularized_corr.columns, np.diag(inverse_corr))],
        )

    X = sm.add_constant(X)
    for y_col in pred_cols:
        print(f"Prediction for {y_col}")
        y = filtered[y_col]
        model = sm.OLS(y, X).fit()
        # model = sm.QuantReg(y, X).fit(q=0.5)  # equiv to min l1 norm the median day
        # model = sm.QuantReg(y, X).fit(q=0.75)  # what predicts a 75th  percentile day?
        print(model.summary())
    return model, filtered


# Example usage
if __name__ == "__main__":
    # This section will run when the script is executed directly

    # Use the predefined variables if running in a notebook where they're already defined
    calendar_df = df.copy()
    work_df = work_data.copy()
    phrase_list = [
        "gym",
        "chores",
        "drink",  #
        "friend",
        "blogs",
        "book",
        "twitter",
        "insta",
        "jack",
        "porn",
        "meditation",  # seems wrong I do 1h less of work when I meditate, not sure what's happening.
    ]

    model, merged_data = regression_predict_work_from_sleep_breakpoints(
        calendar_df=calendar_df,
        work_df=work_df,
        phrase_list=phrase_list,
        use_prev_day=True,
        min_weekly_hours=10,
    )
    if False:  # average differences of X values for each percentile chunk
        X = merged_data.select_dtypes(include=np.number)  # [features].copy()
        pred_cols = ["Hours Working", "work_productivity"]
        X = X.drop(columns=pred_cols + ["Value"])
        for y_name in pred_cols:
            print("\n" + y_name + "\n")
            average_over_percentile_range(X, merged_data[y_name], l1=20, h1=40, l2=60, h2=80)
            average_over_percentile_range(X, merged_data[y_name], l1=10, h1=20, l2=80, h2=90)
    print("\n### Regression by Week  ###\n")
    merged_data.drop(
        columns=["is_weekend", "book_over_1"] + [c for c in merged_data.columns if "days_ago" in c],
        inplace=True,
    )
    agg_funcs = {}
    for col in merged_data.columns:
        if col == "week":
            continue
        elif col == "date":
            agg_funcs[col] = "last"  # Take the last date of the week
        elif col in ("after_hive", "after_mats", "value"):
            # single var is more interpretable, also doesn't change with number of days
            agg_funcs[col] = "mean"
        else:
            # number of days where counts >1
            agg_funcs[col] = "sum"
    # Apply the aggregation
    weekly_data = merged_data.groupby("week").agg(agg_funcs).reset_index()
    print(f"Weekly aggregated data shape: {weekly_data.shape}")
    X = weekly_data.select_dtypes(include=np.number)  # [features].copy()
    pred_cols = ["Hours Working", "work_productivity"]
    X = X.drop(columns=pred_cols + ["Value"])
    X = sm.add_constant(X)
    for y_col in pred_cols:
        print(f"Prediction for {y_col}")
        y = weekly_data[y_col]
        model = sm.OLS(y, X).fit()  # l2
        # model = sm.QuantReg(y, X).fit(q=0.5)  # median
        print(model.summary())

    if False and model is not None:
        # The coefficients tell you how much each feature affects productivity
        print("\nFeature Importance:")
        for feature, coef in zip(model.params.index, model.params):
            if feature != "const":
                sign = "+" if coef > 0 else ""
                print(f"{feature}: {sign}{coef:.4f}")

        # Sort features by absolute importance
        feature_importance = {
            feature: coef
            for feature, coef in zip(model.params.index, model.params)
            if feature != "const"
        }
        sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)

        print("\nFeatures Ranked by Importance (absolute value):")
        for feature, coef in sorted_features:
            sign = "+" if coef > 0 else ""
            print(f"{feature}: {sign}{coef:.4f}")


# %%
import pandas as pd
import numpy as np


def compute_wakeup_to_work_delay(calendar_df):
    # Ensure start and end times are datetimes
    calendar_df["start_time"] = pd.to_datetime(calendar_df["start_time"])
    calendar_df["end_time"] = pd.to_datetime(calendar_df["end_time"])

    # Filter sleep events (assume event_name "sleep" indicates a sleep period)
    sleep_events = calendar_df[calendar_df["event_name"].str.lower() == "sleep"].copy()
    if sleep_events.empty:
        print("No sleep events found.")
        return None

    # For each sleep event, use the end_time as the wakeup time.
    # In case of multiple sleep events in one morning, take the latest end_time.
    sleep_events["wakeup_date"] = sleep_events["end_time"].dt.date
    wakeup_df = sleep_events.groupby("wakeup_date")["end_time"].max().reset_index()
    wakeup_df.rename(columns={"wakeup_date": "date", "end_time": "wakeup_time"}, inplace=True)

    # Filter Clark events that last more than 1 hour
    clark_events = calendar_df[
        (calendar_df["calendar_name"].str.lower() == "clark.benham@gmail.com")
        & (calendar_df["duration"] > 1)
    ].copy()

    # For each wakeup (each day), find the first Clark event that starts after wakeup_time
    delays = []
    for _, row in wakeup_df.iterrows():
        day = row["date"]
        wakeup_time = row["wakeup_time"]
        # Only consider Clark events that are on the same day and after wakeup
        events_after = clark_events[
            (clark_events["start_time"].dt.date == day)
            & (clark_events["start_time"] >= wakeup_time)
        ]
        if not events_after.empty:
            first_work = events_after["start_time"].min()
            delay = (first_work - wakeup_time).total_seconds() / 3600.0  # delay in hours
            delays.append(delay)

    if not delays:
        print("No matching Clark work events found after wakeup.")
        return None

    plt.hist(delays)
    # Compute the 1st, 5th, 10th, and median percentiles
    percentiles = np.percentile(delays, [1, 5, 10, 50])
    result = {
        "1st_percentile": percentiles[0],
        "5th_percentile": percentiles[1],
        "10th_percentile": percentiles[2],
        "median": percentiles[3],
        "all_delays": delays,
    }
    return result


# Example usage:
result = compute_wakeup_to_work_delay(calendar_df)
if result is not None:
    print("Delay percentiles (in hours):")
    print("1st percentile:", result["1st_percentile"])
    print("5th percentile:", result["5th_percentile"])
    print("10th percentile:", result["10th_percentile"])
    print("Median:", result["median"])


# %%
# Calculate the maximum number of days worked and maximum hours worked in any 2-week period from the work data.
def analyze_work_intensity(work_df, window=14):
    # Ensure date is datetime format
    work_df = work_df.copy()
    work_df["date"] = pd.to_datetime(work_df["date"])

    # Create a day indicator column (1 for each day with work)
    work_df["worked_day"] = (work_df["Hours Working"] >= 2).astype(int)

    # Sort by date
    work_df = work_df.sort_values("date")

    # Create a date range covering the entire period
    date_range = pd.date_range(start=work_df["date"].min(), end=work_df["date"].max(), freq="D")

    # Create a complete daily dataframe with zeros for missing days
    daily_df = pd.DataFrame({"date": date_range})
    daily_df = daily_df.merge(
        work_df[["date", "Hours Working", "worked_day"]], on="date", how="left"
    ).fillna(0)

    # Set up 14-day rolling windows (2 weeks)
    rolling_days = daily_df["worked_day"].rolling(window=window).sum()
    rolling_hours = daily_df["Hours Working"].rolling(window=window).sum()

    # Find maximum values
    max_days_worked = rolling_days.max()
    max_hours_worked = rolling_hours.max()

    # Find when these maximums occurred
    max_days_date = daily_df.iloc[rolling_days.argmax()]["date"]
    max_hours_date = daily_df.iloc[rolling_hours.argmax()]["date"]

    return {
        "max_days_worked": max_days_worked,
        "max_days_period_end": max_days_date,
        "max_days_period_start": max_days_date - pd.Timedelta(days=window + 1),
        "max_hours_worked": max_hours_worked,
        "max_hours_period_end": max_hours_date,
        "max_hours_period_start": max_hours_date - pd.Timedelta(days=window + 1),
    }


# To use this function:
window = 35
results = analyze_work_intensity(work_df.tail(700), window)
print(f"Maximum days worked in any {window}-day period: {results['max_days_worked']}")
print(
    f"This occurred in the period: {results['max_days_period_start']} to"
    f" {results['max_days_period_end']}"
)
print(f"Maximum hours worked in any {window}-day period: {results['max_hours_worked']:.1f}")
print(
    f"This occurred in the period: {results['max_hours_period_start']} to"
    f" {results['max_hours_period_end']}"
)
# %%


# Quiting twitter Natural Experiment
def analyze_twitter_vs_time(df, work_data, hide_hours=False):
    """
    Analyzes Twitter usage vs both waste time and useful time during quit and non-quit periods

    Parameters:
    df: calendar events dataframe
    work_data: dataframe with daily work hours and productivity
    hide_hours (bool): If True, hides hour values on y-axis for social media sharing
    """
    # Convert work_data date to datetime for matching
    work_data["date"] = pd.to_datetime(work_data["date"])
    work_data["month"] = work_data["date"].dt.to_period("M")
    work_data["month_str"] = work_data["month"].astype(str)

    # Filter for Twitter events
    twitter_df = df[df["event_name"].str.lower().str.contains("twitter")].copy()
    twitter_df["start_time"] = pd.to_datetime(twitter_df["start_time"])
    twitter_df["date"] = twitter_df["start_time"].dt.date
    twitter_df["month"] = twitter_df["start_time"].dt.to_period("M")

    # Filter for Waste Time events
    waste_df = df[df["calendar_name"] == "Waste Time"].copy()
    waste_df["start_time"] = pd.to_datetime(waste_df["start_time"])
    waste_df["date"] = waste_df["start_time"].dt.date
    waste_df["month"] = waste_df["start_time"].dt.to_period("M")

    # Filter for Useful Time events (work)
    useful_df = df[df["calendar_name"] == "clark.benham@gmail.com"].copy()
    useful_df["start_time"] = pd.to_datetime(useful_df["start_time"])
    useful_df["date"] = useful_df["start_time"].dt.date
    useful_df["month"] = useful_df["start_time"].dt.to_period("M")

    # Get unique dates with events for each month
    all_events_df = pd.concat([twitter_df, waste_df, useful_df])
    all_events_df["date"] = pd.to_datetime(all_events_df["date"])
    all_events_df["month"] = all_events_df["date"].dt.to_period("M")

    # Count actual days with events per month
    days_with_events = all_events_df.groupby("month")["date"].nunique().reset_index()
    days_with_events.columns = ["month", "active_days"]
    days_with_events["month_str"] = days_with_events["month"].astype(str)

    # Group by month for all three event types
    monthly_twitter = twitter_df.groupby("month")["duration"].sum().reset_index()
    monthly_twitter["month_str"] = monthly_twitter["month"].astype(str)

    monthly_waste = waste_df.groupby("month")["duration"].sum().reset_index()
    monthly_waste["month_str"] = monthly_waste["month"].astype(str)

    monthly_useful = useful_df.groupby("month")["duration"].sum().reset_index()
    monthly_useful["month_str"] = monthly_useful["month"].astype(str)

    # Group work_data by month
    monthly_work_hours = work_data.groupby("month_str")["Hours Working"].sum().reset_index()
    monthly_work_productivity = (
        work_data.groupby("month_str")["work_productivity"].sum().reset_index()
    )

    # Merge all the data
    monthly_combined = pd.merge(
        monthly_twitter, monthly_waste, on="month_str", how="outer", suffixes=("_twitter", "_waste")
    )
    monthly_combined = pd.merge(monthly_combined, monthly_useful, on="month_str", how="outer")
    monthly_combined.rename(columns={"duration": "duration_useful"}, inplace=True)

    # Add days with events
    monthly_combined = pd.merge(
        monthly_combined, days_with_events[["month_str", "active_days"]], on="month_str", how="left"
    )

    # Add work hours and productivity
    monthly_combined = pd.merge(monthly_combined, monthly_work_hours, on="month_str", how="left")
    monthly_combined = pd.merge(
        monthly_combined, monthly_work_productivity, on="month_str", how="left"
    )

    monthly_combined.fillna(0, inplace=True)

    # Convert monthly hours to daily hours using actual active days
    monthly_combined["daily_twitter"] = (
        monthly_combined["duration_twitter"] / monthly_combined["active_days"]
    )
    monthly_combined["daily_waste"] = (
        monthly_combined["duration_waste"] / monthly_combined["active_days"]
    )
    monthly_combined["daily_useful"] = (
        monthly_combined["duration_useful"] / monthly_combined["active_days"]
    )
    monthly_combined["daily_work_hours"] = (
        monthly_combined["Hours Working"] / monthly_combined["active_days"]
    )
    monthly_combined["daily_work_productivity"] = (
        monthly_combined["work_productivity"] / monthly_combined["active_days"]
    )

    # Replace NaN with 0 for months with no active days
    monthly_combined = monthly_combined.replace([np.inf, -np.inf], 0)
    monthly_combined = monthly_combined.fillna(0)

    # Define quit periods based on your data
    quit_periods = [
        ("2022-01", "2022-01", "Jan 2022 quit"),
        ("2024-01", "2024-08", "Jan-Aug 2024 quit"),
        ("2025-01", "2025-02", "Jan-Feb 2025 quit"),
    ]

    # Define active periods (between quits)
    active_periods = [
        ("2021-05", "2021-12", "Pre-quit (May-Dec 2021)"),
        ("2022-02", "2023-12", "Active (Feb 2022-Dec 2023)"),
        ("2024-09", "2024-12", "Active (Sep-Dec 2024)"),
        ("2025-03", "2025-12", "Active (Mar 2025+)"),
    ]

    # Calculate statistics for each period type
    quit_stats = []
    active_stats = []

    # Process quit periods
    for start, end, label in quit_periods:
        period_data = monthly_combined[
            (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
        ]

        twitter_hours = period_data["duration_twitter"].sum()
        waste_hours = period_data["duration_waste"].sum()
        useful_hours = period_data["duration_useful"].sum()
        work_hours = period_data["Hours Working"].sum()
        work_productivity = period_data["work_productivity"].sum()
        months_count = len(period_data)

        # Calculate days in period using actual active days
        days_count = period_data["active_days"].sum()

        quit_stats.append(
            {
                "Period": label,
                "Twitter Total": twitter_hours,
                "Waste Total": waste_hours,
                "Useful Total": useful_hours,
                "Work Hours Total": work_hours,
                "Work Productivity Total": work_productivity,
                "Twitter/Day": twitter_hours / days_count if days_count > 0 else 0,
                "Waste/Day": waste_hours / days_count if days_count > 0 else 0,
                "Useful/Day": useful_hours / days_count if days_count > 0 else 0,
                "Work Hours/Day": work_hours / days_count if days_count > 0 else 0,
                "Work Productivity/Day": work_productivity / days_count if days_count > 0 else 0,
                "Months": months_count,
                "Active Days": days_count,
            }
        )

    # Process active periods
    for start, end, label in active_periods:
        period_data = monthly_combined[
            (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
        ]

        twitter_hours = period_data["duration_twitter"].sum()
        waste_hours = period_data["duration_waste"].sum()
        useful_hours = period_data["duration_useful"].sum()
        work_hours = period_data["Hours Working"].sum()
        work_productivity = period_data["work_productivity"].sum()
        months_count = len(period_data)

        # Calculate days in period using actual active days
        days_count = period_data["active_days"].sum()

        active_stats.append(
            {
                "Period": label,
                "Twitter Total": twitter_hours,
                "Waste Total": waste_hours,
                "Useful Total": useful_hours,
                "Work Hours Total": work_hours,
                "Work Productivity Total": work_productivity,
                "Twitter/Day": twitter_hours / days_count if days_count > 0 else 0,
                "Waste/Day": waste_hours / days_count if days_count > 0 else 0,
                "Useful/Day": useful_hours / days_count if days_count > 0 else 0,
                "Work Hours/Day": work_hours / days_count if days_count > 0 else 0,
                "Work Productivity/Day": work_productivity / days_count if days_count > 0 else 0,
                "Months": months_count,
                "Active Days": days_count,
            }
        )

    # Calculate overall statistics
    all_quit_data = pd.concat(
        [
            monthly_combined[
                (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
            ]
            for start, end, _ in quit_periods
        ]
    )

    all_active_data = pd.concat(
        [
            monthly_combined[
                (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
            ]
            for start, end, _ in active_periods
        ]
    )

    # Summary statistics (convert to daily using actual active days)
    quit_days = all_quit_data["active_days"].sum()
    active_days = all_active_data["active_days"].sum()

    quit_twitter_avg = all_quit_data["duration_twitter"].sum() / quit_days if quit_days > 0 else 0
    quit_waste_avg = all_quit_data["duration_waste"].sum() / quit_days if quit_days > 0 else 0
    quit_useful_avg = all_quit_data["duration_useful"].sum() / quit_days if quit_days > 0 else 0
    quit_work_hours_avg = all_quit_data["Hours Working"].sum() / quit_days if quit_days > 0 else 0
    quit_work_productivity_avg = (
        all_quit_data["work_productivity"].sum() / quit_days if quit_days > 0 else 0
    )

    active_twitter_avg = (
        all_active_data["duration_twitter"].sum() / active_days if active_days > 0 else 0
    )
    active_waste_avg = (
        all_active_data["duration_waste"].sum() / active_days if active_days > 0 else 0
    )
    active_useful_avg = (
        all_active_data["duration_useful"].sum() / active_days if active_days > 0 else 0
    )
    active_work_hours_avg = (
        all_active_data["Hours Working"].sum() / active_days if active_days > 0 else 0
    )
    active_work_productivity_avg = (
        all_active_data["work_productivity"].sum() / active_days if active_days > 0 else 0
    )

    # Calculate correlations using daily values
    valid_data = monthly_combined[monthly_combined["active_days"] > 0]
    waste_correlation = valid_data["daily_twitter"].corr(valid_data["daily_waste"])
    useful_correlation = valid_data["daily_twitter"].corr(valid_data["daily_useful"])
    work_hours_correlation = valid_data["daily_twitter"].corr(valid_data["daily_work_hours"])
    work_productivity_correlation = valid_data["daily_twitter"].corr(
        valid_data["daily_work_productivity"]
    )

    # Create separate waste time figure
    fig1, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))

    # Plot 1: Twitter and Waste Time over time (in hours/day)
    ax1.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_twitter"],
        label="Twitter",
        marker="o",
        color="red",
        linewidth=2,
    )
    ax1.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_waste"],
        label="Total Waste Time",
        marker="s",
        color="orange",
        linewidth=2,
    )

    # Highlight quit periods
    for start, end, _ in quit_periods:
        mask = (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
        quit_months = monthly_combined[mask]["month_str"]
        if len(quit_months) > 0:
            ax1.axvspan(
                quit_months.iloc[0],
                quit_months.iloc[-1],
                alpha=0.8,
                color="lightblue",
                label="Twitter Quit Period",
            )

    ax1.set_title("Twitter Usage vs Total Time Wasted Over Time", fontsize=16)
    ax1.set_xlabel("Month", fontsize=12)
    ax1.set_ylabel("Hours/Day", fontsize=12)
    ax1.legend(fontsize=12)
    ax1.grid(True, alpha=0.3)

    # Set x-axis to show every 3rd month
    x_positions = np.arange(len(monthly_combined))
    ax1.set_xticks(x_positions[::3])
    ax1.set_xticklabels(monthly_combined["month_str"].iloc[::3], rotation=45, ha="right")

    if hide_hours:
        ax1.set_yticklabels([])

    # Plot 2: Scatter plot Twitter vs Waste (in hours/day)
    ax2.scatter(
        monthly_combined["daily_twitter"],
        monthly_combined["daily_waste"],
        color="orange",
        s=60,
        alpha=0.7,
    )
    ax2.set_xlabel("Twitter Hours/Day", fontsize=12)
    ax2.set_ylabel("Total Waste Hours/Day", fontsize=12)
    ax2.set_title(
        f"Twitter vs Total Waste Time (Correlation: {waste_correlation:.3f})", fontsize=16
    )
    ax2.grid(True, alpha=0.3)

    if hide_hours:
        ax2.set_xticklabels([])
        ax2.set_yticklabels([])

    plt.tight_layout()
    plt.savefig("twitter_vs_waste_time.png", dpi=300, bbox_inches="tight")
    plt.show()

    # Create separate useful events figure
    fig2, (ax3, ax4) = plt.subplots(2, 1, figsize=(14, 10))

    # Plot 3: Twitter and Useful Events over time (in hours/day)
    ax3.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_twitter"],
        label="Twitter",
        marker="o",
        color="red",
        linewidth=2,
    )
    ax3.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_useful"],
        label="Total Useful Events",
        marker="d",
        color="green",
        linewidth=2,
    )

    # Highlight quit periods
    for start, end, _ in quit_periods:
        mask = (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
        quit_months = monthly_combined[mask]["month_str"]
        if len(quit_months) > 0:
            ax3.axvspan(
                quit_months.iloc[0],
                quit_months.iloc[-1],
                alpha=0.8,
                color="lightblue",
                label="Twitter Quit Period",
            )

    ax3.set_title("Twitter Usage vs Total Useful Events Over Time", fontsize=16)
    ax3.set_xlabel("Month", fontsize=12)
    ax3.set_ylabel("Hours/Day", fontsize=12)
    ax3.legend(fontsize=12)
    ax3.grid(True, alpha=0.3)

    # Set x-axis to show every 3rd month
    ax3.set_xticks(x_positions[::3])
    ax3.set_xticklabels(monthly_combined["month_str"].iloc[::3], rotation=45, ha="right")

    if hide_hours:
        ax3.set_yticklabels([])

    # Plot 4: Scatter plot Twitter vs Useful Events (in hours/day)
    ax4.scatter(
        monthly_combined["daily_twitter"],
        monthly_combined["daily_useful"],
        color="green",
        s=60,
        alpha=0.7,
    )
    ax4.set_xlabel("Twitter Hours/Day", fontsize=12)
    ax4.set_ylabel("Total Useful Events Hours/Day", fontsize=12)
    ax4.set_title(
        f"Twitter vs Total Useful Events (Correlation: {useful_correlation:.3f})", fontsize=16
    )
    ax4.grid(True, alpha=0.3)

    if hide_hours:
        ax4.set_xticklabels([])
        ax4.set_yticklabels([])

    plt.tight_layout()
    plt.savefig("twitter_vs_useful_events.png", dpi=300, bbox_inches="tight")
    plt.show()

    # Create work hours and productivity figure
    fig3, ((ax5, ax6), (ax7, ax8)) = plt.subplots(2, 2, figsize=(16, 12))

    # Plot 5: Twitter and Work Hours over time
    ax5.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_twitter"],
        label="Twitter",
        marker="o",
        color="red",
        linewidth=2,
    )
    ax5.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_work_hours"],
        label="Work Hours",
        marker="^",
        color="blue",
        linewidth=2,
    )

    # Highlight quit periods
    for start, end, _ in quit_periods:
        mask = (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
        quit_months = monthly_combined[mask]["month_str"]
        if len(quit_months) > 0:
            ax5.axvspan(
                quit_months.iloc[0],
                quit_months.iloc[-1],
                alpha=0.8,
                color="lightblue",
                label="Twitter Quit Period",
            )

    ax5.set_title("Twitter Usage vs Work Hours Over Time", fontsize=16)
    ax5.set_xlabel("Month", fontsize=12)
    ax5.set_ylabel("Hours/Day", fontsize=12)
    ax5.legend(fontsize=12)
    ax5.grid(True, alpha=0.3)
    ax5.set_xticks(x_positions[::3])
    ax5.set_xticklabels(monthly_combined["month_str"].iloc[::3], rotation=45, ha="right")

    if hide_hours:
        ax5.set_yticklabels([])

    # Plot 6: Scatter plot Twitter vs Work Hours
    ax6.scatter(
        monthly_combined["daily_twitter"],
        monthly_combined["daily_work_hours"],
        color="blue",
        s=60,
        alpha=0.7,
    )
    ax6.set_xlabel("Twitter Hours/Day", fontsize=12)
    ax6.set_ylabel("Work Hours/Day", fontsize=12)
    ax6.set_title(f"Twitter vs Work Hours (Correlation: {work_hours_correlation:.3f})", fontsize=16)
    ax6.grid(True, alpha=0.3)

    if hide_hours:
        ax6.set_xticklabels([])
        ax6.set_yticklabels([])

    # Plot 7: Twitter and Work Productivity over time
    ax7.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_twitter"],
        label="Twitter",
        marker="o",
        color="red",
        linewidth=2,
    )
    ax7.plot(
        monthly_combined["month_str"],
        monthly_combined["daily_work_productivity"]
        / monthly_combined["daily_work_productivity"].mean(),
        label="Work Productivity",
        marker="h",
        color="purple",
        linewidth=2,
    )

    # Highlight quit periods
    for start, end, _ in quit_periods:
        mask = (monthly_combined["month_str"] >= start) & (monthly_combined["month_str"] <= end)
        quit_months = monthly_combined[mask]["month_str"]
        if len(quit_months) > 0:
            ax7.axvspan(
                quit_months.iloc[0],
                quit_months.iloc[-1],
                alpha=0.8,
                color="lightblue",
                label="Twitter Quit Period",
            )

    ax7.set_title("Twitter Usage vs Work Productivity Over Time", fontsize=16)
    ax7.set_xlabel("Month", fontsize=12)
    ax7.set_ylabel("Units/Day", fontsize=12)
    ax7.legend(fontsize=12)
    ax7.grid(True, alpha=0.3)
    ax7.set_xticks(x_positions[::3])
    ax7.set_xticklabels(monthly_combined["month_str"].iloc[::3], rotation=45, ha="right")

    if hide_hours:
        ax7.set_yticklabels([])

    # Plot 8: Scatter plot Twitter vs Work Productivity
    ax8.scatter(
        monthly_combined["daily_twitter"],
        monthly_combined["daily_work_productivity"],
        color="purple",
        s=60,
        alpha=0.7,
    )
    ax8.set_xlabel("Twitter Hours/Day", fontsize=12)
    ax8.set_ylabel("Work Productivity/Day", fontsize=12)
    ax8.set_title(
        f"Twitter vs Work Productivity (Correlation: {work_productivity_correlation:.3f})",
        fontsize=16,
    )
    ax8.grid(True, alpha=0.3)

    if hide_hours:
        ax8.set_xticklabels([])
        ax8.set_yticklabels([])

    plt.tight_layout()
    plt.savefig("twitter_vs_work_metrics.png", dpi=300, bbox_inches="tight")
    plt.show()

    # Print the formatted summary
    print("TWITTER QUIT ANALYSIS")
    print("=" * 50)
    print(
        f"\nI basically quit Twitter in Jan 2022, from Jan 2024 to Aug 2024, and Jan 2025 to Feb"
        f" 2025."
    )
    print(
        f"\nDaily Twitter usage was {active_twitter_avg:.2f} h/day outside those periods,"
        f" {quit_twitter_avg:.2f} h/day within those periods."
    )
    print(
        f"Total 'Time Wasted' was {active_waste_avg:.2f} h/day normally and"
        f" {quit_waste_avg:.2f} h/day when I quit Twitter."
    )
    print(
        f"Total Useful Events was {active_useful_avg:.2f} h/day normally and"
        f" {quit_useful_avg:.2f} h/day when I quit Twitter."
    )
    print(
        f"Work hours was {active_work_hours_avg:.2f} h/day normally and"
        f" {quit_work_hours_avg:.2f} h/day when I quit Twitter."
    )
    print(
        f"Work productivity was {active_work_productivity_avg:.1f}/day normally and"
        f" {quit_work_productivity_avg:.1f}/day when I quit Twitter."
    )
    print(f"\nCorrelation between Twitter usage and total hours wasted is {waste_correlation:.3f}.")
    print(f"Correlation between Twitter usage and useful events is {useful_correlation:.3f}.")
    print(f"Correlation between Twitter usage and work hours is {work_hours_correlation:.3f}.")
    print(
        "Correlation between Twitter usage and work productivity is"
        f" {work_productivity_correlation:.3f}."
    )
    print("(But I'd default more to natural experiment of quitting Twitter).")

    # Detailed breakdown
    print("\n\nDETAILED BREAKDOWN")
    print("=" * 50)

    print("\nQUIT PERIODS:")
    for stats in quit_stats:
        print(f"\n{stats['Period']}:")
        print(f"  Twitter: {stats['Twitter Total']:.1f} total ({stats['Twitter/Day']:.2f} h/day)")
        print(f"  Waste: {stats['Waste Total']:.1f} total ({stats['Waste/Day']:.2f} h/day)")
        print(f"  Useful: {stats['Useful Total']:.1f} total ({stats['Useful/Day']:.2f} h/day)")
        print(
            f"  Work Hours: {stats['Work Hours Total']:.1f} total"
            f" ({stats['Work Hours/Day']:.2f} h/day)"
        )
        print(
            f"  Work Productivity: {stats['Work Productivity Total']:.1f} total"
            f" ({stats['Work Productivity/Day']:.1f}/day)"
        )
        print(f"  Months: {stats['Months']}, Active Days: {stats['Active Days']}")

    print("\n\nACTIVE PERIODS:")
    for stats in active_stats:
        print(f"\n{stats['Period']}:")
        print(f"  Twitter: {stats['Twitter Total']:.1f} total ({stats['Twitter/Day']:.2f} h/day)")
        print(f"  Waste: {stats['Waste Total']:.1f} total ({stats['Waste/Day']:.2f} h/day)")
        print(f"  Useful: {stats['Useful Total']:.1f} total ({stats['Useful/Day']:.2f} h/day)")
        print(
            f"  Work Hours: {stats['Work Hours Total']:.1f} total"
            f" ({stats['Work Hours/Day']:.2f} h/day)"
        )
        print(
            f"  Work Productivity: {stats['Work Productivity Total']:.1f} total"
            f" ({stats['Work Productivity/Day']:.1f}/day)"
        )
        print(f"  Months: {stats['Months']}, Active Days: {stats['Active Days']}")

    return (
        monthly_combined,
        waste_correlation,
        useful_correlation,
        work_hours_correlation,
        work_productivity_correlation,
    )


# Example usage:
# To create normal graphs with hours shown:
combined_data, waste_corr, useful_corr, work_hours_corr, work_productivity_corr = (
    analyze_twitter_vs_time(
        df.query("start_time >= '2022-01-01T00:00:00+00:00'"), work_data, hide_hours=False
    )
)

# To create graphs for social media (without hour values):
# combined_data, waste_corr, useful_corr, work_hours_corr, work_productivity_corr = analyze_twitter_vs_time(df, work_data, hide_hours=True)


# %%
def amelia_fights(df, print_table=False):
    "Cumulative count and duration of fights"

    target_timestamp = pd.Timestamp("2024-01-01 00:00:00").tz_localize("UTC")  # noqa: F841
    filtered_df = df.query("start_time >= @target_timestamp")

    # All Amelia entries (base set)
    all_amelia = filtered_df[
        filtered_df["event_name"].str.contains("amelia", case=False, na=False)
    ].copy()

    if all_amelia.empty:
        print("No Amelia events found for the selected period.")
        return

    # Regex patterns for interaction categorization
    negative_keywords = ["argue", "fight", "rant", "tiff", "yell", "scream", "flip", "stern"]
    relationship_keywords = [
        "talk",
        "talked",
        "talking",
        "talks",
        "notes",
        "journal",
        "blogs",
        "made up",
        "silence",
        "silience",
        "discussion",
        "write",
        "think",
        "seethe",
        "ruminate",
        "type",
        "writing",
    ]
    negative_pattern = (
        r"(?<!\w)(" + "|".join(re.escape(keyword) for keyword in negative_keywords) + r")(?!\w)"
    )
    relationship_work_pattern = (
        r"(?<!\w)(" + "|".join(re.escape(keyword) for keyword in relationship_keywords) + r")(?!\w)"
    )
    negative_mask = all_amelia["event_name"].str.contains(
        negative_pattern, case=False, regex=True, na=False
    )

    negative_section = all_amelia[negative_mask].copy()

    relationship_work_section = all_amelia[
        all_amelia["event_name"].str.contains(
            relationship_work_pattern,
            case=False,
            regex=True,
            na=False,
        )
        & ~negative_mask
    ].copy()

    # Prepare cumulative data for negative interactions
    negative_section = negative_section.sort_values("start_time").reset_index(drop=True)
    negative_section["cumulative_count"] = range(1, len(negative_section) + 1)
    negative_section["cumulative_duration"] = negative_section["duration"].cumsum()

    # Create monthly aggregations
    all_amelia["year_month"] = all_amelia["start_time"].dt.to_period("M")
    negative_section["year_month"] = negative_section["start_time"].dt.to_period("M")
    relationship_work_section["year_month"] = relationship_work_section["start_time"].dt.to_period(
        "M"
    )

    monthly_all = all_amelia.groupby("year_month")["duration"].sum()
    period_index = monthly_all.index

    monthly_negative = (
        negative_section.groupby("year_month")["duration"].sum().reindex(period_index, fill_value=0)
    )
    monthly_relationship_work = (
        relationship_work_section.groupby("year_month")["duration"]
        .sum()
        .reindex(period_index, fill_value=0)
    )
    monthly_other = (monthly_all - monthly_negative - monthly_relationship_work).clip(lower=0)

    monthly_daily_avg = pd.DataFrame(
        {
            "Negative Interactions": monthly_negative,
            "Relationship Work": monthly_relationship_work,
            "Positive Amelia Events (other)": monthly_other,
        },
        index=period_index,
    )
    days_in_month = pd.Series(period_index.days_in_month, index=period_index, dtype=float)
    latest_timestamp = all_amelia["start_time"].max()
    latest_period = latest_timestamp.to_period("M")
    if latest_period in days_in_month.index:
        if latest_timestamp.tzinfo is None:
            latest_ts_utc = latest_timestamp.tz_localize("UTC")
        else:
            latest_ts_utc = latest_timestamp.tz_convert("UTC")
        start_of_latest = latest_period.to_timestamp().tz_localize("UTC")
        days_elapsed = (latest_ts_utc.normalize() - start_of_latest).days + 1
        days_in_month.loc[latest_period] = max(
            1, min(days_in_month.loc[latest_period], days_elapsed)
        )
    monthly_daily_avg = monthly_daily_avg.div(days_in_month, axis=0)
    monthly_daily_avg.index = monthly_daily_avg.index.to_timestamp()

    # Create plots
    fig = plt.figure(figsize=(14, 12))
    gs = fig.add_gridspec(3, 1, hspace=0.3)

    # Plot 1: Cumulative count
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.plot(
        negative_section["start_time"],
        negative_section["cumulative_count"],
        "b-",
        linewidth=2,
    )
    ax1.set_ylabel("Cumulative Event Count")
    ax1.set_title("Cumulative Number of Amelia Negative Events Over Time")
    ax1.grid(True, alpha=0.3)
    ax1.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
    ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=6))

    # Plot 2: Cumulative duration
    ax2 = fig.add_subplot(gs[1, 0])
    ax2.plot(
        negative_section["start_time"],
        negative_section["cumulative_duration"],
        "r-",
        linewidth=2,
    )
    ax2.set_ylabel("Cumulative Duration (hours)")
    ax2.set_title("Cumulative Time Spent on Amelia Negative Events")
    ax2.grid(True, alpha=0.3)
    ax2.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
    ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=6))

    # Plot 3: Monthly average daily hours stacked bar
    ax3 = fig.add_subplot(gs[2, 0])
    monthly_dates = monthly_daily_avg.index
    negative_values = monthly_daily_avg["Negative Interactions"]
    relationship_values = monthly_daily_avg["Relationship Work"]
    other_values = monthly_daily_avg["Positive Amelia Events (other)"]

    ax3.bar(
        monthly_dates,
        negative_values,
        width=20,
        label="Negative Interactions",
        color="#d62728",
    )
    ax3.bar(
        monthly_dates,
        relationship_values,
        bottom=negative_values,
        width=20,
        label="Relationship Work",
        color="#1f77b4",
    )
    ax3.bar(
        monthly_dates,
        other_values,
        bottom=negative_values + relationship_values,
        width=20,
        label="Positive Amelia Events (other)",
        color="#2ca02c",
    )
    ax3.set_ylabel("Average Daily Hours")
    ax3.set_xlabel("Date")
    ax3.set_title("Average Daily Hours by Category (Monthly)")
    ax3.set_ylim(0, 2.5)
    ax3.legend(loc="best")
    ax3.grid(True, alpha=0.3)
    ax3.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
    ax3.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha="right")

    plt.tight_layout()
    plt.show()

    print(f"Total all Amelia events: {len(all_amelia)}")
    print(f"Total all Amelia duration: {all_amelia['duration'].sum():.2f} hours")
    print(f"Total negative events: {len(negative_section)}")
    print(f"Total negative duration: {negative_section['duration'].sum():.2f} hours")
    print(f"Total relationship work events: {len(relationship_work_section)}")
    print(
        f"Total relationship work duration: {relationship_work_section['duration'].sum():.2f} hours"
    )
    date_range = f"{negative_section['start_time'].min()} to {negative_section['start_time'].max()}"
    print(f"Date range: {date_range}")

    # Create monthly table of unique events with total hours
    # (year_month already created above for monthly aggregations)
    # Normalize event names to lowercase for grouping
    if print_table:
        all_amelia_copy = all_amelia.copy()
        all_amelia_copy["event_name_lower"] = all_amelia_copy["event_name"].str.lower()
        monthly_event_hours = (
            all_amelia_copy.groupby(["year_month", "event_name_lower"])["duration"]
            .sum()
            .reset_index()
        )
        monthly_event_hours = monthly_event_hours.rename(columns={"event_name_lower": "event_name"})
        monthly_event_hours["year_month"] = monthly_event_hours["year_month"].astype(str)

        print("\n" + "=" * 80)
        print("Monthly Breakdown: Total Hours per Unique Event")
        print("=" * 80)
        for month in sorted(monthly_event_hours["year_month"].unique()):
            month_data = monthly_event_hours[monthly_event_hours["year_month"] == month]
            month_data = month_data.sort_values("duration", ascending=False)
            print(f"\n{month}:")
            print("-" * 80)
            print(
                month_data[["event_name", "duration"]].to_string(
                    index=False, float_format=lambda x: f"{x:.2f}"
                )
            )
            print(f"Total for month: {month_data['duration'].sum():.2f} hours")


amelia_fights(df)
